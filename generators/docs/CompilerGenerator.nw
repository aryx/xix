\documentclass[12pt]{report}
%alt: 12pt, twocolumn, landscape

\input{latex/Packages}
\input{latex/Config}
\input{latex/Macros}

%******************************************************************************
% Prelude
%******************************************************************************
%------------------------------------------------------------------------------
%history: 
%------------------------------------------------------------------------------

%thx to LP, changed for the better a few things:
% - use more type aliases instead of (ab)using 'int' everywhere in ocamllex
%   which is not very informative and actually confusing
% - integrate back the simpler lex code generator
% - make more explicit the char_ and its range and the special handling
%   of eof which was spreaded and hidden at many places (with magic numbers
%   like Array.create ... 257)

%thx to codemap/codegraph:

%thx to this manual, better understand lex/yacc:
% - priority of actions when two lex case matches (e.g kwd prio over ident 
%   if case is before)
% - important subtelities of backtracking (or peek) in lex and eof handling
% - important subtelities of eof also in yacc
% - TODO classic s/r conflicts and their explanations on how to understand
%   them and fix them

%history LP-ization:
% - skeleton, mostly copy paste of Template.nw skeleton
% - put all content of files in the Extra section, via 'pfff -lpize'
%   which also now split in chunks!
%    * function, global, struct, enum, constant, macro(actually function)
% - read Extra section, identify concepts, first TOC
% - SEMI distribute parts of the file before
% - TODO nullify, boolify, typeify,    scheckify
% - TODO aspecify advanced features!
% - TODO add figures
% - TODO add explanations

\begin{document}
%******************************************************************************
% Title
%******************************************************************************
\title{
{\Huge 
Principia Softwarica: ([[ocaml]])[[lex]] and ([[ocaml]])[[yacc]]
}\\
{version 0.1}
}

\author{
Yoann Padioleau\\
\texttt{yoann.padioleau@gmail.com}\\
\\
with code from\\
Xavier Leroy 
and Yoann Padioleau
}
% Mike Lesk, Erik schmidt(lex), Xavier Leroy(ocamllex)
% Steve Jonhson and Alfred Aho(yacc), Corbett(byacc), Pad (ocamlyacc)

\maketitle 
\l compiler compilers?


%\onecolumn
\hrule
\begin{quote}
Copyright \copyright{} 2015 Yoann Padioleau \\
All Rights Reserved
\end{quote}
\hrule
%\twocolumn

\begingroup
\hypersetup{linkcolor=blue}
% need to s/onecolumn/twocolumn in report.cls :) for \tableofcontents
\tableofcontents
\endgroup

%******************************************************************************
% Body
%******************************************************************************

\chapter{Introduction}

The goal of this book is to present in full details the source code of
the {lexer} and {parser} code generators
Lex and Yacc.

\section{Motivations}

Why Lex and Yacc? Because I think you are a better programmer if
you fully understand how things work under the hood, and Lex and Yacc
are used by many other programs such as
the C compiler, the assembler, or even the shell.
\l also used in a myriad of DSL (eqn, bc, awk, pic, grap, ...)
\l and other languages (ratfor, f77, cplusplus, ...)
\l Mcillroy says in Unix Readers article that without yacc, lots
\l of unix programs would have not been written or stayed demonstrations.
\l Yacc allowed people to invent and implement languages!

\section{[[ocamllex]] and [[ocamlyacc]]}

% We gonna present ocamllex actually. Shorter code and easier
% to explain than the original lex. And we gonna present
% also a yacc clone written in ocaml (mine), also arguably
% easier to explain (and also to be coherent).

%% More than 15 years ago I didnt think I could write a yacc. It's
%% actually doable. The existing clones are not "unreachable". Their
%% code is not that big. Their algorithms not that hard!

\section{Other compiler generators}

Here are a few compiler generators that I considered for this book, but
which I ultimately discarded:
\begin{itemize}
\item Unix Lex and Yacc
%% note that for once the lex and yacc in Plan9 seems really to
%% come from Unix, with only a small port to Plan9 (as opposed to the other
%% document I've already written which are really Plan9 software)
\item GNU Flex and Bison
% LOC?
\item Antlr
% LOC?
\item Ometa
% ?? Ohm https://ohmlang.github.io/pubs/live2016/
\end{itemize}

% also burg/ocamlburg, ... but not as iconic as lex/yacc

% other lex/yacc clones:
%# C
% - mks lex and yacc? mentionned in {lexyacc} book
% - Unix v6 yacc (in spinnelis unix history repo): 1755 LOC
% - Unix Lex in Plan9 (2800LOC use yacc), Unix Yacc in plan9 (3200LOC in 1 file)
%   yacc in plan9 actually allows action in the middle of the rule
% - Berkeley Yacc (7320LOC)
% - Miniyacc http://c9x.me/yacc/ 1378 LOC
%# ML/haskell
% - ocamllex (756LOC for old simpler version), ocamlyacc (byacc clone 6000LOC)
% - ml-lex (1380LOC, no lex/yacc used), ml-yacc (4700LOC, lex/yacc used)
%   contains some very nice documentation btw, user manuals and docs on the
%   internals, but code is too much functorized and complicated IMHO
% - menhir: 21000 LOC of ocaml, hmm bigger than my fork of ocaml, lol
% - Alex (2900LOC, use lex/yacc), Happy (4900LOC, use lex/yacc)
%# otherx
% - go-yacc! a port of yacc.c to Go by Go authors, might be interesting
%   to contrast C and Go. 3464 LOC so similar.
% - JLex, cup
% - Plex for Rust, LOC?
%   https://github.com/goffrie/plex 
%#research
% - ometa?

%syntax: has multiple backends and nice edebugging output?
%https://medium.com/@DmitrySoshnikov/syntax-language-agnostic-parser-generator-bd24468d7cfc

%todo: implementation techniques
%https://swtch.com/~rsc/regexp/

% LL
% Prolog DCG
% GLR
% PEG
% EDSL for regexps or grammar?


% Many lexers are actually handwritten (e.g. in Plan9), and many grammars
% are also handwritten (seems to have some advantages for IDE, e.g.
% jetbrains does this, but also now hack/flow)

% see also sed/awk? 

\section{Getting started}

\section{Requirements}

% you need to know about lex & yacc :) because we gonne use
% lex and yacc to explain the code of lex and yacc. hmm.

% Dragon book also useful.
%(* To generate directly a NFA from a regular expression.
%   Confer Aho-Sethi-Ullman, dragon book, chap. 3 *)

\section{About this document}
% include "../docs/latex/About.nw"
\input{../docs/latex/About}

\section{Copyright}

Most of this document is actually source code from OCaml, so
those parts are copyright by INRIA.
The prose is mine and is licensed under the GNU Free Documentation
License.

<<copyright ocamllex>>=
(***********************************************************************)
(*                                                                     *)
(*                           Objective Caml                            *)
(*                                                                     *)
(*            Xavier Leroy, projet Cristal, INRIA Rocquencourt         *)
(*                                                                     *)
(*  Copyright 1996 Institut National de Recherche en Informatique et   *)
(*  Automatique.  Distributed only by permission.                      *)
(*                                                                     *)
(***********************************************************************)
@

<<copyright ocamlyacc>>=
(* Yoann Padioleau
 *
 * Copyright (C) 2015 Yoann Padioleau
 *
 * This library is free software; you can redistribute it and/or
 * modify it under the terms of the GNU Lesser General Public License
 * version 2.1 as published by the Free Software Foundation, with the
 * special exception on linking described in file license.txt.
 *
 * This library is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the file
 * license.txt for more details.
 *)
@

\section{Acknowledgments}

% dragon book :)



\chapter{Overview}

\section{Lex and Yacc principles}

% DSL! The first ones!
% Helpers for compiler writers (or nowadays for any tools working
% on source code, e.g. linters, refactorers, smart completion backend, 
% code indexers, etc).

% There is usually a lex/yacc interface they need to agree on,
% but actually lex can also work without yacc (give type of tokens)
% and yacc without lex (write a yylex() function).

%without:
% maybe show trivial parsing by manual recursive descent, each
% rule is a function: expr(), stmt()
%industry: actually many industrial parsers are like that. it has some 
% advantages like simpler error-recovery, better control because yacc
% in the end is quite complex.

\section{[[lex]] command-line interface}

<<[[Main.main()]] print lex usage if wrong number of arguments>>=
if Array.length Sys.argv != 2 then begin
  prerr_endline "Usage: ocamllex <input file>";
  exit 2
end;
@
% .mll to .ml

% diff with traditional lex: not the <xxx>/ thing (but equivalent
% with having multiple rules, which is better I think as it does
% not require a new prefix rule concept).

% diff with traditional yacc: use constructor so can't use
% characters in the grammar as in '+' (since in yacc the token
% is an int/char). But more consistent and cleaner I think.

\section{[[yacc]] command-line interface}

\section{Toy calculator example}

% show generated code too? Can have toy generated code? YES for lex
% at least

% without lex, has to write boilerplate stuff like:
%   case '*':
%       c1 = GETC();
%       if(c1 == '=')
%           return LMLE;
%       break;
% with manual backtracking, lookahead, etc.
% Or look at the code to handle numbers in Compiler.nw, really big.

\section{Code organization}

\section{Software architecture}

\section{Bootstrapping}

% Note that you can bootstrap by yourself manually.
% The .mll and .mly files of ocamllex and ocamlyacc are fairly small
% so you could manually generate the code for them by running
% in your head the generation algorithm. You could also simply entering
% the AST directly corresponding to the .mll and .mly.

\section{Book structure}

%###############################################################################

\part{Lex}

% Essentially a DSL to go from regexps -> automatons (DFA)

% Automaton so given a character it transitions to
% the right state one character at a time and entering certain (final) 
% states triggers a special action (usually returning a token 
% constructor to be then consumed by the yacc parser).

% Why use regexps and not automaton directly? Because regexps are more
% convenient, more declarative! And they can be then compiled 
% to efficient code.

% see Assembler.nw code to handle floats or integers, really
% painful and long! glad we have regexps and not automatons for that.
%less: give both versions, see the productivity win!


\chapter{Core data structures}

\section{Regular expression}

<<type [[Syntax.regular_expression]]>>=
type regular_expression =
    Epsilon
  | Characters of char_ list
  | Sequence of regular_expression * regular_expression
  | Alternative of regular_expression * regular_expression
  | Repetition of regular_expression
@

% Characters could be a combination of Alternative and Char 
% but maybe useful optimisation?

% so 'X?' is really 'X | Epsilon' and 'X+' is 'X X*'

<<type [[Syntax.char_]]>>=
type char_ = int
@
%pad: I added this type.
% why 'int list'? why not 'char list'? because we will use
% eof as a special marker with a special value (256). 
% (But maybe could have used 'char option' where None = eof)
%actually in caml light it's a char list
%but then there was this patch in ocaml 1.06 maybe related:
% * ocamllex: lexers generated by ocamllex can now handle all characters,
%   including '\000'.
% so maybe they used 0 as eof before.

%TODO: toy example? and its dump? unit test (parse string) = AST value?

% In C actually they abuse int to represent tokens and chars together
% so that one can use '(' in the grammar. They can do that by
% starting all special tokens after 256.

\section{Abstract syntax tree}

<<type [[Syntax.lexer_definition]]>>=
type lexer_definition =
    { header: location;
      entrypoints: rule list;
      trailer: location 
    }
@
% have more power than just regexps when can have different entry points! can
% actually do stuff that count! see comments below.
% the list is a kind of Alternative, but it's alternatives with different
% actions.

% in the original lex the rule list is implemented via different
% "prefix" e.g. <COMMENT>/ ... INITIAL/... but ocamllex way
% is cleaner I think. Just reuse a concept we already know, function!

<<type [[Syntax.location]]>>=
type location =
    Location of charpos * charpos
@
% header/trailer/action slice
%(there are so many use of int that it's better to differentiate them)

<<type [[Syntax.charpos]]>>=
type charpos = int
@
%pad: I added this


<<type [[Syntax.rule]]>>=
type rule = string * (regular_expression * action) list
@
%pad: I added this
%less: could have type case = regular_regular * action and so case list

<<type [[Syntax.action]]>>=
type action = location
@
%pad: I added this


%TODO: toy example? and its dump? unit test (parse string) = AST value?


\section{Automata}

% An automata is a set of initial states corresponding to the
% different rules, and a shared transition table, which
% is a kind of matrix

%todo: stateid! like for yacc. Clarify things and more coherent
% what I do in ocamlyacc.

<<type [[Lexgen.automata_entry]]>>=
(* Representation of entry points *)

type automata_entry =
  { auto_name: string;
    auto_initial_state: int;
    auto_actions: (action_id * Ast.action) list;
  }
@
%less: automata_rule? instead of entry? 

% could do type state_id = int; but by now
% we have refined all int types so the remaining ints
% should be all about automaton state ids.

<<type [[Lexgen.action_id]]>>=
type action_id = int
@
%pad: I added this

<<type [[Lexgen.automata_matrix]]>>=
(* indexed by state number *)
type automata_matrix = automata_row array
@
%pad: I added this

<<type [[Lexgen.automata]]>>=
type automata_row =
    Perform of action_id
  (* indexed by an integer between 0 and 256(eof), that is a char_ *)
  | Shift of automata_trans * automata_move array 
@
%old: was automata, but automata_row is more accurate
%val make_dfa: Syntax.lexer_definition -> automata_entry list * automata_matrix

% So array of array when have Shift. Kinda of a matrix with
% row = state and col = character.

%FIGURE: TODO where show example of matrix for a simple regexp/automata

<<type [[Lexgen.automata_trans]]>>=
and automata_trans =
    No_remember
  | Remember of action_id
@
% when there is a rule like "if" { Tif } | ['a'-'z']+ { Tident }
% then once you've read "if", you don't know yet, you still
% need to "shift" but remember that there was a possible
% action if we have to backtrack

<<type [[Lexgen.automata_move]]>>=
and automata_move =
    Backtrack
  | Goto of int
@
% int is another state here, and the index in automata_move array
% is character


% need backtrak, because need lookahead(1) because when
% see "then" you don't know yet if you want to trigger the
% action related to 'then' or look for another character because it
% could be an identifier like 'thenbla'

%TODO: dump example for calc/lexer.mll


\section{Runtime [[lexbuf]]}

% useful to understand how things work internally,
% but also because the action will need to understand partially lexbuf
% (at least use the Lexeme.xxx API functions)

<<type [[Lexing.lexbuf]]>>=
(* The run-time library for lexers generated by ocamllex *)
(* The type of lexer buffers. A lexer buffer is the argument passed
   to the scanning functions defined by the generated scanners.
   The lexer buffer holds the current state of the scanner, plus
   a function to refill the buffer from the input. *)
type lexbuf =
  { refill_buff : lexbuf -> unit;

    mutable lex_buffer : bytes;
    mutable lex_buffer_len : int;

    mutable lex_abs_pos : int;
    mutable lex_start_pos : int;
    mutable lex_curr_pos : int;

    mutable lex_last_pos : int;
    mutable lex_last_action : int;

    mutable lex_eof_reached : bool;

    (* used by lexers generated using the simpler code generation method *)
    mutable lex_last_action_simple : lexbuf -> Obj.t;
  }
@
% lex_last_pos works with  lex_last_action so when backtrack
% you also restore that so you can run the action with the right
% lexeme range

% lex_start_pos is for Lexing.lex_start and also Lexing.lexeme.
% It's set in Lexing.start_lexing each time we call a new
% rule.

%less: put the Lexeme.xxx API functions here?

\chapter{[[main()]]}

<<toplevel [[Main._1]]>>=
let _ = 
  (* Deprecated in OCaml 5 cos done by default: Printexc.catch *)
  main (); 
  exit 0
@


<<function [[Main.main]]>>=
let main () =
  <<[[Main.main()]] print lex usage if wrong number of arguments>>
  let source_name = Sys.argv.(1) in
  let dest_name =
    if Filename.check_suffix source_name ".mll" 
    then Filename.chop_suffix source_name ".mll" ^ ".ml"
    else source_name ^ ".ml" 
  in
  let ic = open_in source_name in
  let oc = open_out dest_name in
  let lexbuf = Lexing.from_channel ic in

  (* parsing *)
  let def =
    try
      Parser.lexer_definition Lexer.main lexbuf
    with exn ->
      close_out oc;
      Sys.remove dest_name;
       <<[[Main.main()]] report error exn>>
      exit 2 
  in
  (* compiling *)
  let (entries, transitions) = Lexgen.make_dfa def in

(* CONFIG
  Output.output_lexdef_simple ic oc 
    def.header (entries, transitions) def.trailer;
*)
  (* optimizing *)
  let tables = Compact.compact_tables transitions in
  (* generating *)
  Output.output_lexdef ic oc def.header tables entries def.trailer;
  close_in ic;
  close_out oc
@

% Lexer.main :) in next section

% Parser.lexer_definition in next next section

<<signature [[Lexgen.make_dfa]]>>=
(* The entry point *)

val make_dfa: Ast.lexer_definition -> automata_entry list * automata_matrix
@

<<signature [[Output.output_lexdef]]>>=
(* Output the DFA tables and its entry points *)

val output_lexdef:
      in_channel -> out_channel ->
      Ast.location (* header *) ->
      Compact.lex_tables ->
      Lexgen.automata_entry list ->
      Ast.location (* trailer *) ->
      unit
@


\chapter{Lexing}
% :) self-reference
% another example of use after calc/lexer.mll :)

%less: might have been better to not use lex so at least
% can see an alternative way to write lexer, that is by hand
% which can help also to understand and better appreciate
% certain things.

% this is the lexer definition of lex lexer definitions ... hmmm

%pad: to please syncweb -to_tex
\ifallcode
<<lex/lexer.ml>>=
@
\fi


<<lex/lexer.mll>>=
<<copyright ocamllex>>
(* The lexical analyzer for lexer definitions. Bootstrapped! *)

{
open Ast
open Parser

exception Lexical_error of string

(* Auxiliaries for the lexical analyzer *)
<<Lexer helper functions and globals>>
}

<<rule Lexer.main>>

<<rule Lexer.action>>

<<rule Lexer.string>>
      
<<rule Lexer.comment>>
@
% can see header, rules, and here actually no trailer.
%    { header: location;
%      entrypoints: (string * rule) list;
%      trailer: location 
%less: no 'let', could be nice to use to illustrate all features :)


% the tokens are defined in parser.mly, but they could be defined in the header
% (like we do in efuns/prog_modes/ocaml_mode.mll for instance)
<<type [[Parser.token]]>>=
%token Trule Tparse Tand
%token <int> Tchar
%token <string> Tstring
%token Tstar Tmaybe Tplus Tor Tlparen Trparen 
%token Tlbracket Trbracket Tcaret Tdash 
%token Tunderscore Teof
%token <Ast.location> Taction
%token Tlet Tequal 
%token <string> Tident
%token Tend  
@
% see parser.mly, it expands in a type token = TIdent of string | ...

<<rule Lexer.main>>=
rule main = parse
  <<[[Lexer.main()]] space case>>
  <<[[Lexer.main()]] comment case>>
  <<[[Lexer.main()]] keyword or identifier case>>
  <<[[Lexer.main()]] string start case>>
  <<[[Lexer.main()]] character cases>>
  <<[[Lexer.main()]] operator cases>>
  <<[[Lexer.main()]] action case>>
  | eof  { Tend }
  | _
    { raise(Lexical_error
             ("illegal character " ^ String.escaped(Lexing.lexeme lexbuf))) }
@

% can see the list inner list
%      entrypoints: (string * (regular_expression * location) list) list;



\section{Comments}

<<[[Lexer.main()]] space case>>=
  [' ' '\010' '\013' '\009' '\012' ] + 
  { main lexbuf }
@
% C-j, C-m, tab, C-l

% so until now was mostly pl-independent, but here we start
% to really be a lex for ocaml
<<[[Lexer.main()]] comment case>>=
| "(*" 
  { comment_depth := 1;
    comment lexbuf;
    main lexbuf }
@

<<Lexer helper functions and globals>>=
let comment_depth = ref 0
@

% regexps can't count, but by playing with multiple
% rules and recursion you can! so good that lex
% is not just about regexps.

<<rule Lexer.comment>>=
and comment = parse
    "(*" 
    { incr comment_depth; comment lexbuf }
  | "*)" 
    { decr comment_depth;
      if !comment_depth == 0 then () else comment lexbuf }
  | '"' 
    { reset_string_buffer();
      string lexbuf;
      reset_string_buffer();
      comment lexbuf }
  | "''"
      { comment lexbuf }
  | "'" [^ '\\' '\''] "'"
      { comment lexbuf }
  | "'\\" ['\\' '\'' 'n' 't' 'b' 'r'] "'"
      { comment lexbuf }
  | "'\\" ['0'-'9'] ['0'-'9'] ['0'-'9'] "'"
      { comment lexbuf }

  | eof 
    { raise(Lexical_error "unterminated comment") }
  | _ 
    { comment lexbuf }
@


% not sure we need that. Actually I find it annoying
%  | '"' 
%    { reset_string_buffer();
%      string lexbuf;
%      reset_string_buffer();
%      comment lexbuf }
%  | "''"
%      { comment lexbuf }
%  | "'" [^ '\\' '\''] "'"
%      { comment lexbuf }
%  | "'\\" ['\\' '\'' 'n' 't' 'b' 'r'] "'"
%      { comment lexbuf }
%  | "'\\" ['0'-'9'] ['0'-'9'] ['0'-'9'] "'"
%      { comment lexbuf }



\section{Keywords and identifiers}

<<[[Lexer.main()]] keyword or identifier case>>=
| ['A'-'Z' 'a'-'z'] ['A'-'Z' 'a'-'z' '\'' '_' '0'-'9'] *
  { match Lexing.lexeme lexbuf with
    | "rule" -> Trule
    | "parse" -> Tparse
    | "and" -> Tand
    | "eof" -> Teof
    | "let" -> Tlet
    | s -> Tident s 
   }
@
% have seen all the keywords (except let)

% could have also actually written as
% | "rule" -> TRule
% ...
% | ['A'-'Z' 'a'-'z'] ...
% but this generates far more states (but maybe it's also more efficient)

%later: there is a 'shortest' keyword in recent ocamllex, probably
% it's like the ? perl greedy operator

%TODO: 'as' is useful

\section{Operators}

<<[[Lexer.main()]] operator cases>>=
| '*'  { Tstar }
| '|'  { Tor }
@
% regexp! see AST correspondance!
% how is sorted priority? in ab*, the star is just for 'b' or for ab?

<<[[Lexer.main()]] operator cases>>=
| '?'  { Tmaybe }
| '+'  { Tplus }
@
% desugared in a a* and  epsilon | a

<<[[Lexer.main()]] operator cases>>=
| '('  { Tlparen }
| ')'  { Trparen }
@



<<[[Lexer.main()]] operator cases>>=
| '['  { Tlbracket }
| ']'  { Trbracket }
| '-'  { Tdash }
| '^'  { Tcaret }
@
% range

<<[[Lexer.main()]] operator cases>>=
| '_'  { Tunderscore }
@



<<[[Lexer.main()]] operator cases>>=
| '='  { Tequal }
@
% for naming

\section{Strings}

<<[[Lexer.main()]] string start case>>=
| '"' 
  { reset_string_buffer();
    string lexbuf;
    Tstring(get_stored_string()) }
@

% use Buffer instead? or just use ^ like I was doing originally?
% actually that's what they do in recent ocamllex

<<Lexer helper functions and globals>>=
let initial_string_buffer = Bytes.create 256
let string_buff = ref initial_string_buffer
let string_index = ref 0
@

<<Lexer helper functions and globals>>=
let reset_string_buffer () =
  string_buff := initial_string_buffer;
  string_index := 0
@

<<Lexer helper functions and globals>>=
let get_stored_string () =
  Bytes.sub_string !string_buff 0 !string_index
@


<<rule Lexer.string>>=
and string = parse
    '"' 
    { () }
  | '\\' [' ' '\010' '\013' '\009' '\026' '\012'] +
    { string lexbuf }
  | '\\' ['\\' '"' 'n' 't' 'b' 'r'] 
    { store_string_char(char_for_backslash(Lexing.lexeme_char lexbuf 1));
      string lexbuf }
  | '\\' ['0'-'9'] ['0'-'9'] ['0'-'9'] 
    { store_string_char(char_for_decimal_code lexbuf 1);
      string lexbuf }
  | eof 
    { raise(Lexical_error "unterminated string") }
  | _ 
    { store_string_char(Lexing.lexeme_char lexbuf 0);
      string lexbuf }
@
% multiline handling with the \ <RETURN> ?
% 10 = C-j, 13 = C-m, 12 = C-l (used in emacs somtimes, supposed to be
%  a page limit or something)
%less: need %9 = Tab? need 12? 26 = C-z, need?

<<Lexer helper functions and globals>>=
let store_string_char c =
  if !string_index >= Bytes.length !string_buff then begin
    let new_buff = Bytes.create (Bytes.length !string_buff * 2) in
    Bytes.blit !string_buff 0 new_buff 0 (Bytes.length !string_buff);
    string_buff := new_buff
  end;
  Bytes.set !string_buff !string_index c;
  incr string_index
@


<<Lexer helper functions and globals>>=
let char_for_backslash = function
    'n' -> '\n'
  | 't' -> '\t'
  | 'b' -> '\b'
  | 'r' -> '\r'
  | c   -> c
@

<<Lexer helper functions and globals>>=
(* note that 48 is the ascii code for '0' hence the substraction below *)
let char_for_decimal_code lexbuf i =
  Char.chr(100 * (Char.code(Lexing.lexeme_char lexbuf i) - 48) +
            10 * (Char.code(Lexing.lexeme_char lexbuf (i+1)) - 48) +
                 (Char.code(Lexing.lexeme_char lexbuf (i+2)) - 48))
@

\section{Characters}

<<[[Lexer.main()]] character cases>>=
| "'" [^ '\\'] "'" 
  { Tchar(Char.code(Lexing.lexeme_char lexbuf 1)) }
| "'" '\\' ['\\' '\'' 'n' 't' 'b' 'r'] "'" 
  { Tchar(Char.code(char_for_backslash (Lexing.lexeme_char lexbuf 2))) }
| "'" '\\' ['0'-'9'] ['0'-'9'] ['0'-'9'] "'" 
  { Tchar(Char.code(char_for_decimal_code lexbuf 2)) }
@

\section{Actions}

<<[[Lexer.main()]] action case>>=
| '{' 
  { let n1 = Lexing.lexeme_end lexbuf in
    brace_depth := 1;
    let n2 = action lexbuf in
    Taction(Location(n1, n2)) }
@


<<Lexer helper functions and globals>>=
let brace_depth = ref 0
@
% need to count them, because the code in action can use '}'

<<rule Lexer.action>>=
and action = parse
    '{' 
    { incr brace_depth;
      action lexbuf }
  | '}' 
    { decr brace_depth;
      if !brace_depth == 0 then Lexing.lexeme_start lexbuf else action lexbuf }
  | '"' 
    { reset_string_buffer();
      string lexbuf;
      reset_string_buffer();
      action lexbuf }
  | "'" [^ '\\'] "'" 
    { action lexbuf }
  | "'" '\\' ['\\' '\'' 'n' 't' 'b' 'r'] "'" 
    { action lexbuf }
  | "'" '\\' ['0'-'9'] ['0'-'9'] ['0'-'9'] "'" 
    { action lexbuf }
  | "(*" 
    { comment_depth := 1;
      comment lexbuf;
      action lexbuf }
  | eof 
    { raise (Lexical_error "unterminated action") }
  | _ 
    { action lexbuf }
@

% not sure you need that, there is anyway no } in strings usually.
%  | '"' 
%    { reset_string_buffer();
%      string lexbuf;
%      reset_string_buffer();
%      action lexbuf }
%  | "'" [^ '\\'] "'" 
%    { action lexbuf }
%  | "'" '\\' ['\\' '\'' 'n' 't' 'b' 'r'] "'" 
%    { action lexbuf }
%  | "'" '\\' ['0'-'9'] ['0'-'9'] ['0'-'9'] "'" 
%    { action lexbuf }

% we need the comment stuff though because the action token is
% also used for the header/trailer and they may contain some comments
% with some } we don't want to consider as the end of the action.

\chapter{Parsing}

% good exercise for next chapter :)

%pad: to please syncweb -to_tex
\ifallcode
<<lex/parser.ml>>=
@
\fi

\section{Overview}

<<lex/parser.mly>>=
<<copyright ocamllex bis>>

/* The grammar for lexer definitions */

%{
open Ast

(* Auxiliaries for the parser. *)
<<parser helper functions and globals>>

%}

/* Tokens */

<<type [[Parser.token]]>>

/* Precedences and associativities. Lower precedences come first. */

<<Parser precedences and associativities>>

/* Entry points */

<<Parser entry points types>>

%%

<<grammar(lex)>>

%%
@
% header, rules, trailer


<<grammar(lex)>>=
<<lex top rule>>

<<lex header rule>>

<<lex named regexp rule>>

<<lex rule rule>>

<<lex regexp rule>>

@

% rule rule :) meta

% no action rule as it's actually just a token

\section{Lexer definition entry point}

<<Parser entry points types>>=
%start lexer_definition
%type <Ast.lexer_definition> lexer_definition
@

<<lex top rule>>=
lexer_definition:
    header named_regexps Trule definition other_definitions header Tend
        { { header = $1;
            entrypoints = $4 :: List.rev $5;
            trailer = $6
           } 
         }
;
@

\section{Header and trailer}

<<lex header rule>>=
header:
    Taction      { $1 }
  | /*epsilon*/  { Location(0,0) }
;
@
%$ 
%an action! something between { }

\section{Lexing rule}

% rule X = parse ...
<<lex rule rule>>=
definition:
    Tident Tequal entry
        { ($1,$3) }
;
entry:
    Tparse case rest_of_entry
        { $2::List.rev $3 }
  | Tparse rest_of_entry
        { List.rev $2 }
;
case:
    regexp Taction
        { ($1,$2) }
;


other_definitions:
    other_definitions Tand definition
        { $3::$1 }
  | /*epsilon*/
        { [] }
;
rest_of_entry:
    rest_of_entry Tor case
        { $3::$1 }
  | /*epsilon*/
        { [] }
;
@
%$


%less: in recent ocamllex the lexer can take more arguments, which is
% useful, for instance in my PHP lexer I pass the beginning symbol
% so that the rule called know what to look for to close it.

\section{Regexps}

\subsection{Basic regexps}

<<Parser precedences and associativities>>=
%left Tor
%left CONCAT
%nonassoc Tmaybe
%left Tstar
%left Tplus
@
%TODO: 15 s/r conflicts right now. Fix them!

% left Tstar?? but it's not a binary operator

<<lex regexp rule>>=
regexp:
    Tchar
        { Characters [$1] }
  | regexp Tstar
        { Repetition $1 }
  | regexp Tor regexp
        { Alternative($1,$3) }
  | regexp regexp %prec CONCAT
        { Sequence($1,$2) }
  | Tlparen regexp Trparen
        { $2 }
  <<rule regexp cases>>
;
@
%$
% need this %prec?

\subsection{Sugar regexps}

<<rule regexp cases>>=
  | regexp Tmaybe
        { Alternative($1, Epsilon) }
  | regexp Tplus
        { Sequence($1, Repetition $1) }
@
%$

\subsection{Range regexps}

<<rule regexp cases>>=
  | Tlbracket char_class Trbracket
        { Characters $2 }
@

<<lex regexp rule>>=
char_class:
    Tcaret char_class1
        { subtract all_chars $2 }
  | char_class1
        { $1 }
;
char_class1:
    Tchar Tdash Tchar
        { char_class $1 $3 }
  | Tchar
        { [$1] }
  | char_class1 char_class1 %prec CONCAT
        { $1 @ $2 }
;

@
%$

<<parser helper functions and globals>>=
@

<<parser helper functions and globals>>=
@
% but not eof!

<<parser helper functions and globals>>=
let rec subtract l1 l2 =
  match l1 with
    [] -> []
  | a::r -> if List.mem a l2 then subtract r l2 else a :: subtract r l2
@


\subsection{Named regexps}
% aliases

<<parser helper functions and globals>>=
let named_regexps =
  (Hashtbl.create 13 : (string, regular_expression) Hashtbl.t)
@

<<rule regexp cases>>=
  | Tident
        { try
            Hashtbl.find named_regexps $1
          with Not_found ->
            prerr_string "Reference to unbound regexp name `";
            prerr_string $1;
            prerr_string "' at char ";
            prerr_int (Parsing.symbol_start());
            prerr_newline();
            exit 2 }
@

<<lex named regexp rule>>=
named_regexps:
    named_regexps Tlet Tident Tequal regexp
        { Hashtbl.add named_regexps $3 $5 }
  | /*epsilon*/
        { () }
;
@


\subsection{Other regexps}

<<rule regexp cases>>=
  | Tstring
        { regexp_for_string $1 }
@
%$

<<parser helper functions and globals>>=
let regexp_for_string s =
  let rec re_string n =
    if n >= String.length s then Epsilon
    else if succ n = String.length s then Characters([Char.code (s.[n])])
    else Sequence(Characters([Char.code (s.[n])]), re_string (succ n))
  in re_string 0
@

<<rule regexp cases>>=
  | Tunderscore
        { Characters all_chars }
  | Teof
        { Characters [Ast.char_eof] }
@
% which is why we use 'int' and not 'char'
% used to be '\000' in older version

\chapter{Checking}

\chapter{Compiling}

% ok so now have the AST of a lexer definition, which are
% multiple "entries" with different cases with regexps and actions.

% there are different techniques to "compile" this:
% - transform first in NFA, then determinize to get a DFA
%   Idea there is to have macrostates where you
%   merge different states in a union state
% - transform in NFA and then run algorithm on this NFA to simulate
%   it (thompson algo?)
% - transfrom from regexp to DFA directly (lex)

<<function [[Lexgen.make_dfa]]>>=
let make_dfa lexdef =
  let (charsets, lex_entries) = 
    encode_lexdef lexdef in
  let (automata_entries, automata_transitions) = 
    encode_lexentries charsets lex_entries in
  automata_entries, automata_transitions
@
%old: was actions ... but confusing with Syntax.action

% each "entry" gets a different automata_entry, but they all share
% the same automata transitions (=~ matrix).

\section{Normalized regexps}
% Normalized ... hmmm unified? singlify?

% goal here is to generate a single regexp for the
% different cases of a single rule, and to put the actions
% in this regexp.


\subsection{Intermediate regexp format}

<<type [[Lexgen.regexp]]>>=
(* Deep abstract syntax for regular expressions *)

type regexp =
    Empty
  | Chars of charset_id
  | Action of action_id
  | Seq of regexp * regexp
  | Alt of regexp * regexp
  | Star of regexp
@
% diff with regular_expression? This is one "unified" regexp where
% the different cases of a rule has been merged and
% where the actions are also merged in! One step closer to
% the unified automaton and action dispatcher we will see later.

<<type [[Lexgen.lexer_entry]]>>=
type lexer_entry =
  { lex_name: string;
    lex_regexp: regexp;
    lex_actions: (action_id * Ast.action) list;
  }
@

<<type [[Lexgen.charset_id]]>>=
type charset_id = int
@
%pad: I added this


<<constant [[Lexgen.chars]]>>=
let chars = ref ([] : char_ list list)
@
<<constant [[Lexgen.chars_count]]>>=
let chars_count = ref (0: charset_id)
@


<<constant [[Lexgen.actions]]>>=
let actions = ref ([] : (action_id * Ast.location) list)
@
% mapping from id to action. really an assoc.
<<constant [[Lexgen.actions_count]]>>=
let actions_count = ref (0: action_id)
@
% as analyze the casedef with the alternatives,
% need to remember the action to run when reach a certain
% "state"

\subsection{[[encode_lexdef()]]}

<<function [[Lexgen.encode_lexdef]]>>=
let encode_lexdef def =
  chars := [];
  chars_count := 0;
(* CONFIG   actions_count := 0; *)
  let entries =
    def.entrypoints |> List.map (fun (entry_name, casedef) ->
        actions := [];
        (* CONFIG !! for simpler output can't do that *)
        actions_count := 0;
        let re = encode_casedef casedef in
        { lex_name = entry_name;
          lex_regexp = re;
          lex_actions = List.rev !actions }
     )
  in
  (* map a charset_id to a charset *)
  let charsets = Array.of_list (List.rev !chars) in
  (charsets, entries)
@
%old: at the end, but redundant I think
%  chars := [];
%  actions := []; (* needed?? *)

% a bit ugly those globals, maybe the function could return
% multiple things

% so will return different entries which contain some
% references to actions in lex_actions or charsets in charsets


\subsection{[[encode_casedef()]]}

<<function [[Lexgen.encode_casedef]]>>=
let encode_casedef casedef =
  casedef |> List.fold_left (fun reg (re, action) ->
     let act_num = !actions_count in
     incr actions_count;
     actions := (act_num, action) :: !actions;
     Alt(reg, Seq(encode_regexp re, Action act_num))
  ) Empty
@

\subsection{[[encode_regexp()]]}

<<function [[Lexgen.encode_regexp]]>>=
let rec encode_regexp = function
    Epsilon -> Empty
  | Characters cl ->
      let n = !chars_count in
      incr chars_count;
      chars := cl :: !chars;
      Chars(n)
  | Sequence(r1,r2) ->
      Seq(encode_regexp r1, encode_regexp r2)
  | Alternative(r1,r2) ->
      Alt(encode_regexp r1, encode_regexp r2)
  | Repetition r ->
      Star (encode_regexp r)
@
% cl are normalized? sorted?

\section{DFA}

% Deterministic Finite Automaton.
% Idea is that have states, and edges/transitions.

%(* To generate directly a NFA from a regular expression.
%   Confer Aho-Sethi-Ullman, dragon book, chap. 3 *)
%less: bad comment? he means DFA not NFA?


<<type [[Lexgen.transition]]>>=
type transition =
    OnChars of charset_id
  | ToAction of action_id
@

<<type [[Lexgen.state]]>>=
type state = transition Set.t
@
% so a state is a place where you are ready to transition over lots 
% of charsets and to perform different actions, and so it's actually
% represented as a set of transition you can do from.
% A macrostate. Remember the NFA to DFA algorithm where you
% merge different state in a union state.
% Convenient because then to know the transition to do from there,
% just look at the value of the state.


<<function [[Lexgen.reset_state_mem]]>>=
let reset_state_mem () =
  state_map := Map.empty;
  Stack.clear todo;
  next_state_num := 0
@

<<constant [[Lexgen.state_map]]>>=
let state_map = ref (Map.empty: (state, int) Map.t)
@



% idea of NFA to DFA is to create macrostates so if
% can from the start either go to state 1 or state 2 reading 'a'
% then you create a macro state 1_and_2.
% similar here?

<<constant [[Lexgen.todo]]>>=
let todo = (Stack.create() : (state * int) Stack.t)
@

<<constant [[Lexgen.next_state_num]]>>=
let next_state_num = ref 0
@



<<function [[Lexgen.encode_lexentries]]>>=
let encode_lexentries charsets lexentries =
  reset_state_mem();

  (* pass 1 on lexentries, get the initial states *)
  let automata_entries =
    lexentries |> List.map (fun entry ->
        { auto_name    = entry.lex_name;
          auto_actions = entry.lex_actions;
          (* get_state() will populate todo and state_map globals *)
          auto_initial_state = get_state (firstpos entry.lex_regexp);
        }
     )
  in
  (* pass 2 on lexentries, get the transitions *)

  let follow = followpos (Array.length charsets) lexentries in
  let states = 
    map_on_all_states (fun st -> translate_state charsets follow st) 
  in
  let transitions = Array.make !next_state_num (Perform 0) in
  states |> List.iter (fun (act, i) -> transitions.(i) <- act);
  (automata_entries, transitions)
@
%note: was 'chars' but prefer charsets
%old:  reset_state_mem(); at the end, but redundant I think
%old: was actions, but again confusing

\subsection{Initial states}

% pos? meh
% but Dragon book used those function names, because indeed it's
% in spirit represents a pos??

% remember that lexdef is a unified regexp so all the cases
% have been merged as an Alt so firstpos will return
% a big set for the beginning.

<<function [[Lexgen.firstpos]]>>=
let rec firstpos = function
    Empty      -> Set.empty
  | Chars pos  -> Set.add (OnChars pos) Set.empty
  | Action act -> Set.add (ToAction act) Set.empty
  | Seq(r1,r2) -> if nullable r1
                  then Set.union (firstpos r1) (firstpos r2)
                  else firstpos r1
  | Alt(r1,r2) -> Set.union (firstpos r1) (firstpos r2)
  | Star r     -> firstpos r
@

<<function [[Lexgen.nullable]]>>=
let rec nullable = function
    Empty      -> true
  | Chars _    -> false
  | Action _   -> false
  | Seq(r1,r2) -> nullable r1 && nullable r2
  | Alt(r1,r2) -> nullable r1 || nullable r2
  | Star _r     -> true
@

<<function [[Lexgen.get_state]]>>=
let get_state st = 
  try
    Map.find st !state_map
  with Not_found ->
    let num = !next_state_num in
    incr next_state_num;
    state_map := Map.add st num !state_map;
    Stack.push (st, num) todo;
    num
@

% so if regexp = "ab"
% then firstpos = OnChars 'a' which will then be mapped to
% a state 0?

% what todo is for? see below.

\subsection{Transitions}

<<function [[Lexgen.map_on_all_states]]>>=
let map_on_all_states f =
  let res = ref [] in
  begin try
    while true do
      let (st, i) = Stack.pop todo in
      let r = f st in
      res := (r, i) :: !res
    done
  with Stack.Empty -> ()
  end;
  !res
@

<<function [[Lexgen.translate_state]]>>=
let translate_state charsets follow = 
 fun state ->
  match split_trans_set state with
    (n, []) -> Perform n
  | (n, ps) -> Shift((if n = no_action then No_remember else Remember n),
                     transition_from charsets follow ps)
@

% split transition set, one action and a union of charsets
<<function [[Lexgen.split_trans_set]]>>=
let split_trans_set trans_set =
  Set.fold (fun trans (act, pos_set as act_pos_set) ->
      match trans with
        OnChars pos -> (act, pos :: pos_set)
      | ToAction act1 -> if act1 < act then (act1, pos_set) else act_pos_set
   ) trans_set (no_action, [])
@
% first action has priority!!! lexing rule! the order matters!
% so if do  "if" and later an ident rule, then the first
% action will be executed

<<constant [[Lexgen.no_action]]>>=
let no_action = (max_int: action_id)
@
% use max_int because do < above.


% the meat! finally!
% why 257? 0-255 for chars, then 256 for eof! so length = 257.
<<function [[Lexgen.transition_from]]>>=
let transition_from charsets follow pos_set = 
  let tr    = Array.make (Ast.charset_size + 1) Set.empty in
  pos_set |> List.iter (fun pos ->
     charsets.(pos) |> List.iter (fun c ->
           tr.(c) <- Set.union tr.(c) follow.(pos)
      )
  );

  let shift = Array.make (Ast.charset_size + 1) Backtrack in
  for i = 0 to Ast.charset_size do
    shift.(i) <- goto_state tr.(i)
  done;
  shift
@

<<function [[Lexgen.goto_state]]>>=
let goto_state st =
  if Set.is_empty st 
  then Backtrack 
  (* can create a new state in todo *)
  else Goto (get_state st)
@


<<function [[Lexgen.followpos]]>>=
let followpos size_charsets entries =
  let v = Array.make size_charsets Set.empty in
  let fill_pos first = function
      OnChars pos -> v.(pos) <- Set.union first v.(pos)
    | ToAction _  -> () 
  in
  let rec fill = function
      Seq(r1,r2) ->
        fill r1; fill r2;
        Set.iter (fill_pos (firstpos r2)) (lastpos r1)
    | Alt(r1,r2) ->
        fill r1; fill r2
    | Star r ->
        fill r;
        Set.iter (fill_pos (firstpos r)) (lastpos r)
    | _ -> () in
  entries |> List.iter (fun entry -> fill entry.lex_regexp);
  v
@
%TODOL ??? need the Dragon Book I think.


<<function [[Lexgen.lastpos]]>>=
let rec lastpos = function
    Empty      -> Set.empty
  | Chars pos  -> Set.add (OnChars pos) Set.empty
  | Action act -> Set.add (ToAction act) Set.empty
  | Seq(r1,r2) -> if nullable r2
                  then Set.union (lastpos r1) (lastpos r2)
                  else lastpos r2
  | Alt(r1,r2) -> Set.union (lastpos r1) (lastpos r2)
  | Star r     -> lastpos r
@


\chapter{Generating}

% caml light actually generate a set of functions

%val output_lexdef:
%      in_channel -> out_channel ->
%      Syntax.location (* header *) ->
%      Compact.lex_tables ->
%      Lexgen.automata_entry list ->
%      Syntax.location (* trailer *) ->
%      unit

%TODO: how hard to generate instead for C code? so can replace
% Unix lex!
%TODO: can simplify and use automata_matrix instead of compacted
% lex_tables? and use ocaml code to run the engine?

% pretty simple, header, shared tables, rules, trailer
<<function [[Output.output_lexdef]]>>=
(* Main output function *)

let output_lexdef ic oc header tables entry_points trailer =
  <<[[Output.output_lexdef()]] print statistics>>
  copy_chunk ic oc header;
  output_tables oc tables;
  <<[[Output.output_lexdef()]] generate entry points>>
  copy_chunk ic oc trailer
@


<<[[Output.output_lexdef()]] print statistics>>=
Printf.printf "%d states, %d transitions, table size %d bytes\n"
  (Array.length tables.tbl_base)
  (Array.length tables.tbl_trans)
  (2 * (Array.length tables.tbl_base + Array.length tables.tbl_backtrk +
        Array.length tables.tbl_default + Array.length tables.tbl_trans +
        Array.length tables.tbl_check));
flush stdout;
@

\section{Entry points}

<<[[Output.output_lexdef()]] generate entry points>>=
(match entry_points with
  [] -> ()
| entry1 :: entries ->
    output_string oc "let rec "; 
    output_entry ic oc entry1;
    entries |> List.iter (fun e -> 
      output_string oc "and "; 
      output_entry ic oc e
    )
);
@

<<function [[Output.output_entry]]>>=
(* Output the entries *)

let output_entry ic oc e =
  fprintf oc "%s lexbuf = %s_rec lexbuf %d\n"
          e.auto_name e.auto_name e.auto_initial_state;
  fprintf oc "and %s_rec lexbuf state =\n" e.auto_name;
  fprintf oc "  match Lexing.engine lex_tables state lexbuf with\n    ";
  let first = ref true in
  e.auto_actions |> List.iter (fun (num, loc_action) ->
      if !first 
      then first := false 
      else fprintf oc "  | ";
      fprintf oc "%d -> (" num;
      copy_chunk ic oc loc_action;
      fprintf oc ")\n"
  );
  fprintf oc "  | n -> lexbuf.Lexing.refill_buff lexbuf; %s_rec lexbuf n\n\n"
          e.auto_name
@

\section{Shared transition table}

<<function [[Output.output_tables]]>>=
(* Output the tables *)

let output_tables oc tbl =
  output_string oc "let lex_tables = {\n";
  fprintf oc "  Lexing.lex_base = \n%a;\n" output_array tbl.tbl_base;
  fprintf oc "  Lexing.lex_backtrk = \n%a;\n" output_array tbl.tbl_backtrk;
  fprintf oc "  Lexing.lex_default = \n%a;\n" output_array tbl.tbl_default;
  fprintf oc "  Lexing.lex_trans = \n%a;\n" output_array tbl.tbl_trans;
  fprintf oc "  Lexing.lex_check = \n%a\n" output_array tbl.tbl_check;
  output_string oc "}\n\n"
@

<<function [[Output.output_array]]>>=
let output_array oc v =
  output_string oc "   \"";
  for i = 0 to Array.length v - 1 do
    output_byte oc (v.(i) land 0xFF);
    output_byte oc ((v.(i) asr 8) land 0xFF);
    if i land 7 = 7 then output_string oc "\\\n    "
  done;
  output_string oc "\""
@

<<function [[Output.output_byte]]>>=
(* To output an array of short ints, encoded as a string *)

let output_byte oc b =
  output_char oc '\\';
  output_char oc (Char.chr(48 + b / 100));
  output_char oc (Char.chr(48 + (b / 10) mod 10));
  output_char oc (Char.chr(48 + b mod 10))
@

\section{Chunks}

<<function [[Output.copy_chunk]]>>=
let copy_chunk ic oc (Location(start,stop)) =
  seek_in ic start;
  let n = ref (stop - start) in
  while !n > 0 do
    let m = input ic copy_buffer 0 (min !n 1024) in
    output oc copy_buffer 0 m;
    n := !n - m
  done
@

<<constant [[Output.copy_buffer]]>>=
(* To copy the ML code fragments *)

let copy_buffer = Bytes.create 1024
@


%<<constant Output.copy_chunk>>=
%let copy_chunk =
%  match Sys.os_type with
%  | _       -> copy_chunk_unix
%@


\chapter{Running}











%###############################################################################


\part{Yacc}

% essentially a DSL to go from CFG grammar -> pushdown automaton

\chapter{Core data structures}

\section{Context free grammar}

<<type [[Ast.grammar]](yacc)>>=
type grammar = rule list
@

<<type [[Ast.rule_]](yacc)>>=
  and rule = {
    lhs: nonterm;
    rhs: symbol list;
    act: action;
  }
@

<<type [[Ast.term]](yacc)>>=
(* uppercase string *)
type term = T of string
@

<<type [[Ast.nonterm]](yacc)>>=
(* lowercase string *)
type nonterm = NT of string
@

<<type [[Ast.symbol]](yacc)>>=
type symbol = Term of term | Nonterm of nonterm
@

<<type [[Ast.charpos]](yacc)>>=
type charpos = int
@

<<type [[Ast.location]](yacc)>>=
type location =
    Location of charpos * charpos
@
%$

<<type [[Ast.action]](yacc)>>=
(* the slice may contain the special $<digit> markers *)
type action = location
@
%$

<<constant [[Ast.noloc]](yacc)>>=
let noloc = Location(0, 0)
@


%EXAMPLE:
<<constant [[Tests.arith]](yacc)>>=
(* from tests/yacc/arith.mly which is a copy of the representative grammar in
 * the dragon book in 4.1
 * $S -> E         (R0)
 * E -> E + T | T  (R1, R2)
 * T -> T * F | F  (R3, R4)
 * F -> ( E ) | id (R5, R6)
 *)
let arith =
    [{lhs = NT "e";
      rhs = [Nonterm (NT "e"); Term (T "PLUS");  Nonterm (NT "t")];
      act = noloc};
     {lhs = NT "e"; 
      rhs = [Nonterm (NT "t")];
      act = noloc};
     {lhs = NT "t";
      rhs = [Nonterm (NT "t"); Term (T "MULT"); Nonterm (NT "f")];
      act = noloc};
     {lhs = NT "t"; rhs = [Nonterm (NT "f")];
      act = noloc};
     {lhs = NT "f";
      rhs = [Term (T "TOPAR"); Nonterm (NT "e"); Term (T "TCPAR")];
      act = noloc};
     {lhs = NT "f"; 
      rhs = [Term (T "ID")];
      act = noloc}]
(*
let augmented_arith =
  {lhs = NT "$S"; rhs = [Nonterm (NT "e")]; act = noloc} :: arith
*)
@
%$


\section{Abstract syntax tree}

<<type [[Ast.parser_definition]](yacc)>>=
(* main data structure *)
type parser_definition = {
  header: location;
  directives: directive list;
  grm: grammar;
  trailer: location;
}
@

<<type [[Ast.directive]](yacc)>>=
type directive =
  | Token of type_ option * term
  | Start of nonterm
  | Type of type_ * nonterm
  | Prec of unit (* TODO *)
@

<<type [[Ast.type_]](yacc)>>=
  and type_ = string
@



\section{Pushdown automata}
% bottom-up LR parsing

<<type [[Lr0.stateid]](yacc)>>=
type stateid = S of int
@
% like in an automaton! a state

<<type [[Lrtables.lr_tables]](yacc)>>=
type lr_tables = action_table * goto_table
@


<<type [[Lrtables.action_table]](yacc)>>=
(* term can be also the special "$" terminal.
 * Everything not in the list is an Error action, so this
 * list should not contain any Error.
 *)
type action_table = 
    ((Lr0.stateid * Ast.term) * action) list
@
%$

<<type [[Lrtables.action]](yacc)>>=
type action =
  | Shift of Lr0.stateid
  | Reduce of Lr0.ruleidx
  | Accept
  | Error
@
% shift/reduce! push on stack, pop on stack

<<type [[Lrtables.goto_table]](yacc)>>=
type goto_table = 
    ((Lr0.stateid * Ast.nonterm) * Lr0.stateid) list
@

<<type [[Lr0.ruleidx]](yacc)>>=
(* the index of the rule in env.g *)
type ruleidx = R of int
@






<<type [[Lr0.env]](yacc)>>=
type env = {
  (* augmented grammar where r0 is $S -> start_original_grammar *)
  g: Ast.rule array;
}
@
%$

<<constant [[Ast.start_nonterminal]](yacc)>>=
(* They should not conflict with user-defined terminals or non terminals
 * because nonterminals cannot contain '$' according to lexer.mll and 
 * terminals must start with an uppercase letter according again
 * to lexer.mll
 *)
let start_nonterminal = NT "S$"
@

<<constant [[Ast.dollar_terminal]](yacc)>>=
let dollar_terminal = T "$"
@
%$


\section{Runtime [[lr_tables]] and [[parser_env]]}


<<type [[Parsing.lr_tables]](yacc)>>=
type 'tok lr_tables = {
  action: stateid * 'tok -> action;
  goto: stateid * nonterm -> stateid;
}
@

% similar to previous section, but here it's DS used by
% the generated code while before it's DS used by the code generator
% (which happens to coincide sometimes but for optimisation purpose
% we should actually really use an int or an indexed nonterminal
% instead of a string)
<<type [[Parsing.stateid]](yacc)>>=
type stateid = S of int
@

<<type [[Parsing.nonterm]](yacc)>>=
(* less: could be an index, but easier for debug to use the original name *)
type nonterm = NT of string
@


<<type [[Parsing.action]](yacc)>>=
type action = 
  | Shift of stateid
  | Reduce of nonterm * int (* size of rhs of the rule *) * rule_action
  | Accept
@
% no Error, instead will have Parse_error exn
<<exception [[Parsing.Parse_error]](yacc)>>=
exception Parse_error
        (* Raised when a parser encounters a syntax error.
           Can also be raised from the action part of a grammar rule,
           to initiate error recovery. *)
@


<<type [[Parsing.rule_action]](yacc)>>=
(* index in the rule actions table passed to yyparse *)
type rule_action = RA of int
@


<<type [[Parsing.rules_actions]](yacc)>>=
type rules_actions = (parser_env_simple -> Obj.t) array
@


<<signature [[Parsing.yyparse_simple]](yacc)>>=
val yyparse_simple:
  'tok lr_tables -> rules_actions ->
  (Lexing.lexbuf -> 'tok) -> ('tok -> string) -> Lexing.lexbuf -> 'a
@



<<type [[Parsing.parser_env_simple]](yacc)>>=
type parser_env_simple = {
  states: stateid Stack.t;
  (* todo: opti: could use a growing array as one oftens needs to index it
   * with the peek_val
   * The semantic attributes (token value or non terminal value).
   *)
  values: Obj.t Stack.t;
  mutable current_rule_len: int;
}
@
% pushdown automata!




%EXAMPLE:
<<type [[Tests.token]](yacc)>>=
type token =
  | T0
  | TEOF
@

<<function [[Tests.test_lr_engine]](yacc)>>=
(* what we should generate *)
let test_lr_engine () =
  let tokens = ref [T0; TEOF] in
  let lexbuf = Lexing.from_string "fake" in
  
  let lexfun _lexbuf =
    match !tokens with
    | [] -> failwith "no more tokens"
    | x::xs ->
      tokens := xs;
      x
  in
  let lrtables = {
    Parsing_.action = (function 
      | (S 0, T0) -> Shift (S 1)
      | (S 1, TEOF) -> Reduce (NT "S", 1, RA 1)
      | (S 2, TEOF) -> Accept
      | _ -> raise Parsing.Parse_error
    );
    Parsing_.goto = (function
      | S 0, NT "S" -> S 2
      | _ -> raise Parsing_.Parse_error
    );
  }
  in
  (* todo *)
  let rules_action = [||] in
  let string_of_tok = function | T0 -> "T0" | TEOF -> "TEOF" in
  
  Parsing_.yyparse_simple lrtables rules_action lexfun string_of_tok lexbuf
@

\chapter{[[main()]]}

<<function [[Main.main]](yacc)>>=
let main () =

  if Array.length Sys.argv != 2 then begin
    prerr_endline "Usage: ocamlyacc <input file>";
    exit 2
  end;

  let source_name = Sys.argv.(1) in

  let dest_name =
    if Filename.check_suffix source_name ".mly" 
    then Filename.chop_suffix source_name ".mly" ^ ".ml"
    else source_name ^ ".ml" 
  in
  let ic = open_in source_name in
  let lexbuf = Lexing.from_channel ic in

  (* parsing *)
  let def =
    try
      Parser.parser_definition Lexer.main lexbuf
    with exn ->
      Sys.remove dest_name;
       (match exn with
         Parsing.Parse_error ->
           prerr_string "Syntax error around char ";
           prerr_int (Lexing.lexeme_start lexbuf);
           prerr_endline "."
       | Lexer.Lexical_error s ->
           prerr_string "Lexical error around char ";
           prerr_int (Lexing.lexeme_start lexbuf);
           prerr_string ": ";
           prerr_string s;
           prerr_endline "."
       | _ -> raise exn
       );
      exit 2 
  in
  let env = Lr0.mk_env_augmented_grammar (Ast.start_symbol def) def.grm  in
  let automaton = Lr0.canonical_lr0_automaton env in
  Dump.dump_lr0_automaton env automaton;
  
  let (first, eps) = First_follow.compute_first def.grm in
  let follow = First_follow.compute_follow env (first, eps) in
  let tables = Slr.lr_tables env automaton follow in
  Dump.dump_lrtables env tables;

  let oc = open_out dest_name in
  Output.output_parser def env tables ic oc;
  close_out oc;
  ()
@

<<toplevel [[Main._1]](yacc)>>=
let _ = 
(*
  Tests.test_lr0 ();
  Tests.test_first_follow ();
  Tests.test_slr ();
  Tests.test_lr_engine ();
*)
  (*Printexc.catch*) main (); 
  exit 0
@


<<function [[Ast.start_symbol]](yacc)>>=
let start_symbol def =
  try 
    (match
      def.directives |> List.find (function
        | Start _x -> true
        | _ -> false
      )
     with
     | Start x -> x
     | _ -> failwith "impossible"
    )
  with Not_found -> failwith "no start symbol found"
@



\chapter{Lexing}

<<yacc/lexer.mll>>=
{
<<copyright ocamlyacc>>

open Ast (* Location *)
open Parser

exception Lexical_error of string

<<Lexer helper functions and globals(yacc)>>
}

<<rule Lexer.main(yacc)>>

<<rule Lexer.action(yacc)>>

<<rule Lexer.comment(yacc)>>

<<rule Lexer.angle(yacc)>>

<<backward compatible lexing rules(yacc)>>
@


<<type [[Parser.token]](yacc)>>=
%token <Ast.term> TTerm
%token <Ast.nonterm> TNonterm
%token Ttoken Tprec Tstart Ttype
%token TColon TOr TSemicolon
%token <string> TAngle
%token <Ast.location> TAction
%token TEOF
@
% see parser.mly, it expands in a type token = TIdent of string | ...



<<rule Lexer.main(yacc)>>=
rule main = parse
  <<[[Lexer.main()]] space case (yacc)>>
  <<[[Lexer.main()]] comment case (yacc)>>
  <<[[Lexer.main()]] keyword or identifier case (yacc)>>
  <<[[Lexer.main()]] action case (yacc)>>
  <<[[Lexer.main()]] operator cases (yacc)>>
  <<[[Lexer.main()]] type case (yacc)>>
  <<[[Lexer.main()]] backward compatible cases (yacc)>>
| eof  { TEOF }
| _
    { raise(Lexical_error
             ("illegal character " ^ String.escaped(Lexing.lexeme lexbuf))) }
@



\section{Comments}

<<[[Lexer.main()]] space case (yacc)>>=
| [' ' '\010' '\013' '\009' '\012' ] + 
    { main lexbuf }
@

<<[[Lexer.main()]] comment case (yacc)>>=
| "(*" 
    { comment_depth := 1;
      comment lexbuf;
      main lexbuf }
@

<<Lexer helper functions and globals(yacc)>>=
let comment_depth = ref 0
@

<<rule Lexer.comment(yacc)>>=
and comment = parse
| "(*" 
    { incr comment_depth; 
      comment lexbuf }
| "*)" 
    { decr comment_depth;
      if !comment_depth == 0 
      then () 
      else comment lexbuf }

| eof { raise(Lexical_error "unterminated comment") }
| _ { comment lexbuf }
@

\section{Keywords and identifiers}

<<[[Lexer.main()]] keyword or identifier case (yacc)>>=
(* terminals, uppercase *)
| ['A'-'Z' ] ['A'-'Z' 'a'-'z' '\'' '_' '0'-'9'] *
    { TTerm (T (Lexing.lexeme lexbuf)) }
(* nonterminals, lowercase *)
| ['a'-'z' ] ['A'-'Z' 'a'-'z' '\'' '_' '0'-'9'] *
    { TNonterm (NT (Lexing.lexeme lexbuf)) }
(* directives, % prefixed *)
| '%' ['A'-'Z' 'a'-'z'] ['A'-'Z' 'a'-'z' '\'' '_' '0'-'9'] *
    { match Lexing.lexeme lexbuf with
      | "%token" -> Ttoken
      | "%prec" -> Tprec
      | "%start" -> Tstart
      | "%type" -> Ttype
      | s -> failwith ("Unknown directive: " ^ s)
    }
@

\section{Operators}

<<[[Lexer.main()]] operator cases (yacc)>>=
| ':'  { TColon }
| '|'  { TOr }
| ';'  { TSemicolon }
@


\section{Actions}

<<[[Lexer.main()]] action case (yacc)>>=
(* actions and header/trailer *)
| '{' 
    { let n1 = Lexing.lexeme_end lexbuf in
      brace_depth := 1;
      let n2 = action lexbuf in
      TAction(Location(n1, n2)) }
@

<<Lexer helper functions and globals(yacc)>>=
let brace_depth = ref 0
@


<<rule Lexer.action(yacc)>>=
(* TODO: handle $x *)
and action = parse
| '{' 
    { incr brace_depth;
      action lexbuf }
| '}' 
    { decr brace_depth;
      if !brace_depth == 0 
      then Lexing.lexeme_start lexbuf 
      else action lexbuf }
| "(*" 
    { comment_depth := 1;
      comment lexbuf;
      action lexbuf }

| eof { raise (Lexical_error "unterminated action") }
| _   { action lexbuf }
@
%$

\section{Types}

<<[[Lexer.main()]] type case (yacc)>>=
(* for types *)
| '<'  { TAngle (angle lexbuf) }
@

<<rule Lexer.angle(yacc)>>=
and angle = parse
| '>' { "" }
| eof { raise(Lexical_error "unterminated type") }
| [^'>']+ { let s = Lexing.lexeme lexbuf in s ^ angle lexbuf }
| _ { let s = Lexing.lexeme lexbuf in s ^ angle lexbuf }
@


\chapter{Parsing}
% :) self-reference

% this is the parser definition of yacc parser definitions ... hmmm

%(* The lexical analyzer for lexer definitions. Bootstrapped! *)


\section{Overview}

<<yacc/parser.mly>>=
%{
<<copyright ocamlyacc>>
open Ast

%}

<<type [[Parser.token]](yacc)>>

<<Parser entry point definition(yacc)>>

%%

<<grammar(yacc)>>

@

<<grammar(yacc)>>=
<<yacc top rule>>

<<yacc header rule>>

<<yacc directive rule>>

<<yacc grammar rule>>

<<yacc rule rule>>

<<yacc extra rules>>
@
% rule rule :) again

\section{Parser definition entry point}

<<Parser entry point definition(yacc)>>=
%start parser_definition
%type <Ast.parser_definition> parser_definition
@

<<yacc top rule>>=
parser_definition: header directives_opt grammar header_opt TEOF 
  { { header = $1; directives = $2; grm = $3; trailer = $4 } }
;
@



\section{Header and trailer}

<<yacc header rule>>=
header: TAction { $1 }
;
@
%$


<<yacc extra rules>>=
header_opt:
          { Ast.noloc }
 | header { $1 }  
;
@
%$


\section{Grammar rule}

<<yacc grammar rule>>=
grammar: rules_opt { $1 }
;
@
%$

<<yacc rule rule>>=
rule_: TNonterm TColon cases TSemicolon 
  { $3 |> List.map (fun (case, action) -> 
    { lhs = $1; rhs = case; act = action }) 
  }
;
@
% rule rule :)

<<yacc rule rule>>=
cases: 
   symbols_opt TAction           { [$1, $2] }
 | symbols_opt TAction TOr cases { ($1, $2)::$4 }
;
@
%$

<<yacc rule rule>>=
symbol: 
   TTerm    { Term $1 }
 | TNonterm { Nonterm $1 }
;
@


<<yacc extra rules>>=
rules_opt: 
                   { [] }
 | rule_ rules_opt { $1 @ $2 }
;
symbols_opt:
                      { [] }
 | symbol symbols_opt { $1::$2 }
;
@

\section{Directives}

<<yacc directive rule>>=
directive:
   Ttoken type_opt terms { $3 |> List.map (fun t -> Token ($2, t)) }
 | Tprec                 { [Prec ()] }
 | Tstart TNonterm       { [Start $2] }
 | Ttype TAngle TNonterm { [Type ($2, $3)] }
;
@
%$


<<yacc extra rules>>=
directives_opt:
                            { [] }
 | directive directives_opt { $1 @ $2 }
;
type_opt:
          { None }
 | TAngle { Some $1 }  
;
terms: TTerm terms_opt { $1::$2 }
;

terms_opt:
                   { [] }
 | TTerm terms_opt { $1::$2 }
;
@
%$



\chapter{Checking}

<<type [[Check.error]](yacc)>>=
type error = unit
@

<<exception [[Check.Error]](yacc)>>=
exception Error of error
@

<<signature [[Check.check]](yacc)>>=
val check: Ast.parser_definition -> unit
@

<<function [[Check.check]](yacc)>>=
let check def =
  failwith "TODO"
@


\chapter{Compiling}

% this would be so much easier in prolog ...
% all those first, follow, lr0, which are really about
% set manipulations and exploring space, this could be done more
% succintely in prolog

\section{LR0 automaton}

%EXAMPLE:
<<function [[Tests.test_lr0]](yacc)>>=
let test_lr0 () =
  let env = Lr0.mk_env_augmented_grammar (NT "e") arith in

  (* closure *)
  let items = Set.singleton (R 0, D 0) in
  let i0 = Lr0.closure env items in
  let _xs = Set.elements i0 in
  (* [(R 0, D 0); (R 1, D 0); (R 2, D 0); (R 3, D 0); (R 4, D 0); (R 5, D 0);
   (R 6, D 0)] *)
  
  (* goto *)
  let items = Set.of_list [(R 0, D 1); (R 1, D 1)] in
  let i6 = Lr0.goto env items (Term (T "PLUS")) in
  let _xs = Set.elements i6 in
  (* [(R 1, D 2); (R 3, D 0); (R 4, D 0); (R 5, D 0); (R 6, D 0)] *)
  ()
@
%$

\subsection{[[automaton]]}

<<type [[Lr0.automaton]](yacc)>>=
type automaton = {
  states: items Set_.t;
  (* state 0 is the starting state *)
  int_to_state: items array;
  state_to_int: (items, stateid) Map_.t;
  (* goto mapping *)
  trans: (items * Ast.symbol, items) Map_.t;
}
@

<<type [[Lr0.items]](yacc)>>=
(* a.k.a an LR0 state *)
type items = item Set_.t
@


<<type [[Lr0.item]](yacc)>>=
(* as mentionned in the dragon book *)
type item = ruleidx * dotidx
@


<<type [[Lr0.dotidx]](yacc)>>=
(* the dot position in the rhs of a rule *)
type dotidx = D of int
@


\subsection{[[closure()]]}

<<signature [[Lr0.closure]](yacc)>>=
val closure: env -> items -> items
@

<<function [[Lr0.closure]](yacc)>>=
(* todo: opti: use kernel items *)
let closure env items =
  let result = ref items in

  let added = ref true in
  while !added do
    added := false;

    !result |> Set.iter (fun item ->
      let (R ridx), didx = item in
      let r = env.g.(ridx) in

      match after_dot r didx with
      | Some (Nonterm b) ->
          let rules_idx = rules_of b env in
          rules_idx |> List.iter (fun ridx ->
            let item = (ridx, D 0) in
            if not (Set.mem item !result) then begin
              added := true;
              result := Set.add item !result;
            end
          )
      | _ -> ()
    )
  done;
  !result
@


<<signature [[Lr0.after_dot]](yacc)>>=
val after_dot: Ast.rule -> dotidx -> Ast.symbol option
@
<<function [[Lr0.after_dot]](yacc)>>=
let after_dot r (D idx) =
  try Some (List.nth r.rhs idx)
  with Failure _ -> None
@


<<function [[Lr0.rules_of]](yacc)>>=
let rules_of nt env =
  let res = ref [] in
  env.g |> Array.iteri (fun idx r ->
    if r.lhs = nt
    then res := (R idx) :: !res
  );
  List.rev !res
@


\subsection{[[goto()]]}

<<signature [[Lr0.goto]](yacc)>>=
val goto: env -> items -> Ast.symbol -> items
@

<<function [[Lr0.goto]](yacc)>>=
let goto env items symbol =
  let start =
    Set.fold (fun item acc ->
      let (R ridx), didx = item in
      let r = env.g.(ridx) in
      match after_dot r didx with
      | Some symbol2 when symbol = symbol2 ->
          Set.add (R ridx, move_dot_right didx) acc
      | _ -> acc
    ) items Set.empty
  in
  closure env start
@

<<function [[Lr0.move_dot_right]](yacc)>>=
let move_dot_right (D idx) = 
  (D (idx + 1))
@


\subsection{[[canonical_lr0_automaton()]]}

<<signature [[Lr0.mk_env_augmented_grammar]](yacc)>>=
val mk_env_augmented_grammar: Ast.nonterm (* start *) -> Ast.grammar -> env
@
<<function [[Lr0.mk_env_augmented_grammar]](yacc)>>=
let mk_env_augmented_grammar start xs =
  let noloc = Location (0, 0) in
  let start = {lhs = Ast.start_nonterminal; rhs =[Nonterm start]; act=noloc} in
  { g = Array.of_list (start::xs) }
@


<<signature [[Lr0.canonical_lr0_automaton]](yacc)>>=
(* assumes augmented grammar *)
val canonical_lr0_automaton: env -> automaton
@

<<function [[Lr0.canonical_lr0_automaton]](yacc)>>=
let canonical_lr0_automaton env =
  let start_item = (R 0, D 0) in
  let start_items = closure env (Set.singleton start_item) in
  let result = ref (Set.singleton start_items) in
  let transitions = ref Map.empty in
  let symbols = all_symbols env in
  
  let added = ref true in
  while !added do
    added := false;

    !result |> Set.iter (fun items ->
      symbols |> Set.iter (fun symb ->
        let itemset = goto env items symb in
        if not (Set.is_empty itemset) && not (Map.mem (items, symb)!transitions)
        then transitions := Map.add (items, symb) itemset !transitions;

        if not (Set.is_empty itemset) && not (Set.mem itemset !result) 
        then begin
          added := true;
          result := Set.add itemset !result;
        end
      )
    )
  done;
  let states = !result in
  let trans = !transitions in

  (* put start state first in the list of states *)
  let states_without_start = Set.remove start_items states |> Set.elements  in
  let states_list = start_items::states_without_start in

  let int_to_items = states_list |> Array.of_list in
  let items_to_int = 
    let x = ref Map.empty in
    int_to_items |> Array.iteri (fun i items ->
      x := Map.add items (S i) !x
    );
    !x
  in
  { states = states;
    int_to_state = int_to_items;
    state_to_int = items_to_int;
    trans = trans
  }
@


<<signature [[Lr0.all_symbols]](yacc)>>=
val all_symbols: env -> Ast.symbol Set_.t
@
<<function [[Lr0.all_symbols]](yacc)>>=
let all_symbols env =
  env.g |> Array.fold_left (fun acc r ->
    ((Nonterm r.lhs)::r.rhs) |> List.fold_left (fun acc symbol ->
      Set.add symbol acc
      ) acc
  ) Set.empty
@

\section{SLR tables}

<<signature [[Slr.lr_tables]](yacc)>>=
val lr_tables: 
  Lr0.env -> Lr0.automaton -> First_follow.follow -> Lrtables.lr_tables
@


%EXAMPLE:
<<function [[Tests.test_slr]](yacc)>>=
let test_slr () =
  let env = Lr0.mk_env_augmented_grammar (NT "e") arith in

  (* automaton *)
  let auto = Lr0.canonical_lr0_automaton env in
  
  (* first, follow *)
  let (first, eps) = First_follow.compute_first arith in
  let follow = First_follow.compute_follow env (first, eps) in
  
  (* slr tables *)
  let tables = Slr.lr_tables env auto follow in

  Dump.dump_lrtables env tables;
  ()
@

\subsection{[[first]]}

<<constant [[Tests.arith_ll]](yacc)>>=
(*
 * E -> E E'
 * E' -> + T E' | epsilon
 * T -> F T'
 * T' -> * F T' | epsilon
 * F -> ( E ) | id
 *)
let arith_ll = 
    [
     {lhs = NT "e";
      rhs = [Nonterm (NT "t"); Nonterm (NT "e'")];
      act = noloc};
     {lhs = NT "e'"; 
      rhs = [Term (T "PLUS"); Nonterm (NT "t"); Nonterm (NT "e'")];
      act = noloc};
     {lhs = NT "e'"; 
      rhs = [];
      act = noloc};
     {lhs = NT "t";
      rhs = [Nonterm (NT "f"); Nonterm (NT "t'")];
      act = noloc};
     {lhs = NT "t'"; 
      rhs = [Term (T "MULT"); Nonterm (NT "f"); Nonterm (NT "t'")];
      act = noloc};
     {lhs = NT "t'"; 
      rhs = [];
      act = noloc};
     {lhs = NT "f";
      rhs = [Term (T "TOPAR"); Nonterm (NT "e"); Term (T "TCPAR")];
      act = noloc};
     {lhs = NT "f"; 
      rhs = [Term (T "ID")];
      act = noloc}]
@

<<function [[Tests.test_first_follow]](yacc)>>=
let test_first_follow () =
  let (first, eps) = First_follow.compute_first arith_ll in
  let _first' = first |> Map.to_list |> List.map (fun (t, set) -> 
    t, Set.elements set)
  in
  let _eps' = Set.elements eps in

  let env = Lr0.mk_env_augmented_grammar (NT "e") arith_ll in
  let follow = First_follow.compute_follow env (first, eps) in
  let _follow' = follow |> Map.to_list |> List.map (fun (t, set) ->
    t, Set.elements set)
  in
  ()
@




<<type [[First_follow.first]](yacc)>>=
type first = (Ast.symbol, Ast.term Set_.t) Map_.t
@

<<type [[First_follow.epsilon]](yacc)>>=
type epsilon = Ast.nonterm Set_.t
@

<<signature [[First_follow.compute_first]](yacc)>>=
val compute_first: Ast.grammar -> first * epsilon
@

<<function [[First_follow.compute_first]](yacc)>>=
let compute_first grm =
  (* faster to use an hashtbl? could use Hashtbl.replace which would
   * be faster?
   *)
  let first = ref Map.empty in
  let epsilon = ref Set.empty in

  (* initialize first *)
  grm |> List.iter (fun r ->
    if not (Map.mem (Nonterm r.lhs) !first)
    then first := !first |> Map.add (Nonterm r.lhs) Set.empty ;
    r.rhs |> List.iter (function
      | Term t ->
        if not (Map.mem (Term t) !first)
        then first := !first |> Map.add (Term t) (Set.singleton t);
      | Nonterm _  -> ()
    )
  );

  (* fixpoint *)
  let added = ref true in
  while !added do
    added := false;

    grm |> List.iter (fun r ->
      let lhs = Nonterm r.lhs in
      if r.rhs = [] && not (Set.mem r.lhs !epsilon)
      then begin 
        added := true;
        epsilon := Set.add r.lhs !epsilon;
      end;

      let rec aux all_before_are_nullable xs =
        match xs with
        | [] -> 
            if all_before_are_nullable && not (Set.mem r.lhs !epsilon)
            then begin 
              epsilon := Set.add r.lhs !epsilon;
              added := true;
            end
        | x::xs when all_before_are_nullable ->
            (* less: if check.ml use/def check has been done correctly,
             * we should never get some Not_found here
             *)
             let first_x = try Map.find x !first with Not_found -> Set.empty in
             let old = try Map.find lhs !first with Not_found -> Set.empty in
             if not (Set.is_empty first_x) && 
                not (Set.subset first_x old)
             then begin 
               first := !first |> Map.add(Nonterm r.lhs)(Set.union old first_x);
               added := true
             end;
             (match x with
             | Term _ -> (* we can stop here *) ()
             | Nonterm nt ->
                if Set.mem nt !epsilon
                then aux true xs
             )
        | _ -> ()
      in
      aux true r.rhs
    )
  done;
  !first, !epsilon
@


\subsection{[[follow]]}

<<type [[First_follow.follow]](yacc)>>=
type follow = (Ast.nonterm, Ast.term Set_.t) Map_.t
@

<<signature [[First_follow.compute_follow]](yacc)>>=
(* take Lr0.env for its augmented grammar *)
val compute_follow: Lr0.env -> first * epsilon -> follow
@

<<function [[First_follow.compute_follow]](yacc)>>=
(* assumes augmented grammar *)
let compute_follow env (first, epsilon) =
  let follow = ref Map.empty in

  (* initialize first *)
  env.g |> Array.iter (fun r ->
    if not (Map.mem r.lhs !follow)
    then follow := !follow |> Map.add r.lhs Set.empty ;
  );
  (* follow($S) = { $ } *)
  follow := !follow |> Map.add Ast.start_nonterminal 
      (Set.singleton Ast.dollar_terminal);

  (* fixpoint *)
  let added = ref true in
  while !added do
    added := false;

    env.g |> Array.iter (fun r ->
      let rec aux xs =
        match xs with
        | [] -> ()
        | x::beta ->
          (match x with
          (* A -> alpha B beta *)
          | Nonterm b ->
              let set = first_of_sequence (first, epsilon) beta in
              let oldb = Map.find b !follow in
              if not (Set.is_empty set) &&
                 not (Set.subset set oldb) then begin
                  follow := !follow |> Map.add b (Set.union oldb set);
                  added := true
                end;
              if epsilon_of_sequence epsilon beta
              then begin
                let oldb = Map.find b !follow in
                let follow_a = Map.find r.lhs !follow in
                if not (Set.is_empty follow_a) &&
                   not (Set.subset follow_a oldb) then begin
                     follow := !follow |> Map.add b (Set.union oldb follow_a);
                     added := true
                   end;
              end;
              aux beta
          | Term _ -> aux beta
          )
      in
      aux r.rhs
    )
  done;
  !follow
@



<<function [[First_follow.first_of_sequence]](yacc)>>=
let rec first_of_sequence (first, epsilon) xs =
  match xs with
  | [] -> Set.empty
  | x::xs ->
      let set = Map.find x first in
      (match x with
      | Term _ -> set
      | Nonterm nt ->
          if Set.mem nt epsilon
          then Set.union set (first_of_sequence (first, epsilon) xs)
          else set
      )
@

<<function [[First_follow.epsilon_of_sequence]](yacc)>>=
let rec epsilon_of_sequence epsilon xs =
  match xs with
  | [] -> true
  | x::xs ->
      (match x with
      | Term _ -> false
      | Nonterm nt ->
          Set.mem nt epsilon && epsilon_of_sequence epsilon xs
      )
@

\subsection{[[Slr.lr_tables()]]}

<<function [[Slr.lr_tables]](yacc)>>=
let lr_tables env auto follow =
  let trans = auto.trans |> Map.to_list in

  let action_tables =
    Map.fold (fun items stateid acc ->
      Set.fold (fun item acc ->
        let (R ridx, didx) = item in
        let r = env.g.(ridx) in
        match Lr0.after_dot r didx with
        (* a shift *)
        | Some (Term t) -> 
            let items2 = Map.find (items, (Term t)) auto.trans in
            let dst = Map.find items2 auto.state_to_int in
            ((stateid, t), Shift dst)::acc
        | Some (Nonterm _) -> acc
        (* a reduction *)
        | None -> 
            if r.lhs = Ast.start_nonterminal
            then ((stateid, Ast.dollar_terminal), Accept)::acc
            else
              let terms = Map.find r.lhs follow in
              let xs = Set.elements terms in
              (xs |> List.map (fun t -> (stateid, t), Reduce (R ridx))) @ acc
      ) items acc
    ) auto.state_to_int []
  in


  let goto_tables =
    trans |> map_filter (fun ((items1, symb), items2) ->
      match symb with
      | Nonterm nt ->
        let src = Map.find items1 auto.state_to_int in
        let dst = Map.find items2 auto.state_to_int in
        Some ((src, nt), dst)
      | _ -> None
    )
  in

  action_tables, goto_tables
@



\chapter{Generating}

<<signature [[Output.output_parser]](yacc)>>=
val output_parser: 
  Ast.parser_definition -> Lr0.env -> Lrtables.lr_tables -> 
  in_channel -> out_channel -> unit
@

<<function [[Output.output_parser]](yacc)>>=
let output_parser def env lrtables ic oc =
  let pf x = Printf.fprintf oc x in
  let (action_table, goto_table) = lrtables in

  let htype = Hashtbl.create 13 in
  def.directives |> List.iter (function
    | Token (Some s, t) -> Hashtbl.add htype (Term t) s
    | Type (s, nt) -> Hashtbl.add htype (Nonterm nt) s
    | _ -> ()
  );

  pf "type token =\n";
  def.directives |> List.iter (function
    | Token (sopt, (T s)) ->
      pf " | %s%s\n" s
        (match sopt with
        | None -> ""
        | Some s -> spf " of %s" s
        )
    | _ -> ()
  );

  copy_chunk ic oc def.header;
  pf "\n";
  pf "let user_actions = [|\n";
  env.g |> Array.iteri (fun i r ->
    if i = 0
    then pf "  (fun __parser_env -> failwith \"parser\");\n"
    else begin
      let s = get_chunk ic r.act in
      (* ugly: right now have to be before replace_dollar_underscore
       * because replace_dollar_underscore does side effect on s
       *)
      let dollars = extract_dollars_set s in
      let s' = replace_dollar_underscore s in
      let symbs = Array.of_list (Nonterm (NT "_fake_"):: r.rhs) in
      pf "  (fun __parser_env -> \n";
      dollars |> Set.iter (fun i ->
        pf "     let _%d = (Parsing.peek_val_simple __parser_env %d : %s) in\n"
          i i
          (* type info on terminal or on non terminal *)
          (let symb = symbs.(i) in
           if not (Hashtbl.mem htype symb)
           then
             (match symb with
             | Nonterm (NT s) -> "'" ^ s
             | Term _ -> failwith "you try to access a token with no value"
             )
           else Hashtbl.find htype symb
          )
      );

      pf "    Obj.repr((\n";
      pf "      "; 
      pf "%s" s';
      pf "   )";
      if Hashtbl.mem htype (Nonterm r.lhs)
      then pf ": %s" (Hashtbl.find htype (Nonterm r.lhs))
      else ();
      pf ")\n";
      pf "   );\n";
    end
  );
  pf "|]\n";
  copy_chunk ic oc def.trailer;


  pf "\n";
  pf "open Parsing\n";

  (* for debugging support *)
  pf "let string_of_token = function\n";
  def.directives |> List.iter (function
    | Token (sopt, (T s)) ->
      pf " | %s%s -> \"%s\"\n" s
        (match sopt with
        | None -> ""
        | Some _s -> " _"
        )
        s
    | _ -> ()
  );
  pf "\n";

  (* the main tables *)
  pf "let lrtables = {\n";

  (* the action table *)
  pf "  action = (function\n";
  action_table |> List.iter (fun ((S id, T t), action) ->
    (* if reached a state where there is dollar involved, means
     * we're ok!
     *)
    if t = "$"
    then begin 
      (* in practice the '_' will be the TEOF token returned another time
       * by the lexer.
       *)
      pf "   | S %d, _ -> " id;
      (match action with
      | Reduce (R ridx) ->
          let r = env.g.(ridx) in
          let n = List.length r.rhs in
          let (NT l) = r.lhs in
          pf "Reduce (NT \"%s\", %d, RA %d)" l n ridx
      | Accept -> pf "Accept"
      | _ -> failwith "impossible to have a non reduce or accept action on $"
      );
      pf "\n";
    end
    else begin
      pf "   | S %d, %s%s -> " id t
        (if Hashtbl.mem htype (Term (T t)) 
         then " _"
         else ""
        );
      (match action with
      | Shift (S id) -> pf "Shift (S %d)" id
      | Accept -> pf "Accept"
      | Reduce (R ridx) ->
          let r = env.g.(ridx) in
          let n = List.length r.rhs in
          let (NT l) = r.lhs in
          pf "Reduce (NT \"%s\", %d, RA %d)" l n ridx
      | Error -> failwith "Error should not be in action tables"
      );
      pf "\n";
    end
  );

  pf "    | _ -> raise Parse_error\n";
  pf "  );\n";


  (* the goto table *)
  pf "  goto = (function\n";
  goto_table |> List.iter (fun ((S id1, NT nt), S id2) ->
    pf "  | S %d, NT \"%s\" -> S %d\n" id1 nt id2
  );
  pf "    | _ -> raise Parse_error\n";
  pf "  );\n";
  pf "}\n";


  (* the main entry point *)
  let nt = Ast.start_symbol def in
  let (NT start) = nt in

  pf "let %s lexfun lexbuf =\n" start;
  pf "  Parsing.yyparse_simple lrtables user_actions lexfun string_of_token lexbuf\n";
  ()
@
%ugly-c: power of higher order function here, can pass lexfun, no need
% for ugly hard-coded yylex function

\section{Entry point}
%less: should be entry points at some point

\section{LR tables}

\section{User actions}

\subsection{Text extraction}
<<constant [[Output.copy_buffer]](yacc)>>=
let copy_buffer = Bytes.create 1024
@

<<function [[Output.get_chunk]](yacc)>>=
let get_chunk ic (Location(start,stop)) =
  seek_in ic start;
  let buf = Buffer.create 1024 in

  let n = ref (stop - start) in
  while !n > 0 do
    let m = input ic copy_buffer 0 (min !n 1024) in
    Buffer.add_string buf (Bytes.sub_string copy_buffer 0 m);
    n := !n - m
  done;
  Buffer.contents buf
@

<<function [[Output.copy_chunk]](yacc)>>=
let copy_chunk ic oc (Location(start,stop)) =
  seek_in ic start;
  let n = ref (stop - start) in
  while !n > 0 do
    let m = input ic copy_buffer 0 (min !n 1024) in
    output oc copy_buffer 0 m;
    n := !n - m
  done
@

\subsection{Dollar substitutions}

<<function [[Output.replace_dollar_underscore]](yacc)>>=
(* less: we could use Str instead, but this adds a dependency.
 *  Str.global_replace (Str.regexp_string "$") "_" s
 *)
let replace_dollar_underscore s =
  let buf = Bytes.of_string s in
  let rec aux startpos =
    try
      let idx = Bytes.index_from buf startpos '$' in
      Bytes.set buf idx '_';
      aux (idx+1)
    with Not_found -> ()
  in
  aux 0;
  Bytes.to_string buf
@


\subsection{Semantic values managment}

<<function [[Output.int_of_char]](yacc)>>=
(* todo: what about $22? what error message give if type and $x ? *)
let int_of_char c =
  let i = Char.code c in
  if i <= Char.code '9' && i >= Char.code '1'
  then i - Char.code '0'
  else failwith (spf "the characted %c is not a char" c)
@

<<function [[Output.extract_dollars_set]](yacc)>>=
let extract_dollars_set s =
  let set = ref Set.empty in
  let rec aux startpos =
    try
      let idx = String.index_from s startpos '$' in
      let c = String.get s (idx + 1) in
      set := Set.add (int_of_char c) !set;
      aux (idx + 2)
    with Not_found | Invalid_argument _ | Failure _ -> ()
  in
  aux 0;
  !set
@
%$

\chapter{Running}

<<function [[Parsing.yyparse_simple]](yacc)>>=
let yyparse_simple lrtables rules_actions lexfun string_of_tok lexbuf =

  let env = {
    states = Stack.create ();
    values = Stack.create ();
    current_rule_len = 0;
  }
  in
  
  env.states |> Stack.push (S 0);
  let a = ref (lexfun lexbuf) in

  let finished = ref false in
  let res = ref (Obj.repr ()) in
  while not !finished do
    
    let s = Stack.top env.states in
    log (spf "state %d, tok = %s" (let (S x) = s in x) (string_of_tok !a));

    match lrtables.action (s, !a) with
    | Shift t ->
        log (spf "shift to %d" (let (S x) = t in x));
        env.states |> Stack.push t;
        env.values |> Stack.push (value_of_tok (Obj.repr !a));
        a := lexfun lexbuf;
    | Reduce (nt, n, ra) ->
        for i = 1 to n do
          Stack.pop env.states |> ignore
        done;
        let s = Stack.top env.states in
        env.states |> Stack.push (lrtables.goto (s, nt));
        let (NT ntstr) = nt in
        let (RA raidx) = ra in
        env.current_rule_len <- n;
        let v = rules_actions.(raidx) env in
        for i = 1 to n do
          Stack.pop env.values |> ignore
        done;
        env.values |> Stack.push v;
        log (spf "reduce %s, ra = %d" ntstr raidx);
    | Accept ->
        log "done!";
        finished := true;
        res := Stack.top env.values;
  done;
  Obj.magic !res
@


<<signature [[Parsing.peek_val_simple]](yacc)>>=
val peek_val_simple: parser_env_simple -> int -> 'a
@

<<function [[Parsing.peek_val_simple]](yacc)>>=
let peek_val_simple env i =
  if i < 1 && i >= env.current_rule_len
  then failwith (spf "peek_val_simple invalid argument %d" i)
  else Obj.magic (Common.Stack_.nth (env.current_rule_len - i) env.values)
@

<<function [[Parsing.value_of_tok]](yacc)>>=
(* hmm, imitate what is done in parsing.c. A big ugly but tricky
 * to do otherwise and have a generic LR parsing engine.
 *)
let value_of_tok t =
  if Obj.is_block t
  then Obj.field t 0
  else Obj.repr ()
@

\part{Advanced Topics}

\chapter{Advanced Features}

% for lex: 'as' which apparently comes with a lot more code

\section{[[as]]}

\section{Characters as terminals}
% in the grammar. '(' expr ')'. Nice but can in C but not in ocaml
% (or will need tricks)

\section{[[left]], [[right]], [[assoc]]}

\section{Action in the middle of a rule}
% plan9 yacc in Compiler.nw allows this.
%todo: how it can work??

\chapter{Error managment Support}

% with 5a,if write SWI 0 and does not support such instruction in 5a then get
% hello2.s:16 syntax error, last name: SWI
% line: inst.;
% saw $
% nice :)
% trace of non-terminal!


\section{Positions tracking}
% for error managment
% related to Debugging support, but not completely. It's more
% about "error support"

% programmer using lex/yacc wants that so that when user
% use program generated using lex/yacc he can get error reports
% indicating the position in the input where he made a mistake

% Lexing.lexeme_char
% Parsing.symbol_start

% tricky to have symbol_start? need to keep some bookkeeping

\section{Error recovery}

\chapter{Debugging Support}

% see also my debug flag in ocamllex, hmm but this flag is more
% for debugging my own code right now


\section{[[#line]]}

% useful to generate some special directives in the generated code
% that indicates from where certain parts of the code comes from
% so that error report can redirect to the original file instead
% of the generated one (that we don't want the programmer to modify)

\section{Traces}

<<constant [[Parsing.debug]](yacc)>>=
let debug = ref true
@

<<function [[Parsing.log]](yacc)>>=
let log x = 
  if !debug
  then begin
    print_endline ("YACC: " ^ x); flush stdout
  end
@


% see also OCAMLRUNPARAM=p for yacc

\section{[[parser.output]]}

% imitate menhir? better report?

% example of issues and explanations? (see my conflicts.txt files in pfff),
%  - dangling else
%  - ...
%  - conflicts in Tiger? 
%      xs: x | x xs   and x: FUNCTION id
%    then there will be a s/r on FUNCTION

\chapter{Optimisations}

\section{Lex}

\subsection{Compacted automata}

% Idea is that the matrix has lots of repetitions.
% Mostly goes to the same state for a large range of character
% (which will be stored in  'default' below)

%apparently in latest ocamllex they optimized even more and use 
% "Extension to tagged automata - Ville Larikari"


<<type [[Compact.lex_tables]]>>=
(* Compaction of an automata *)

type lex_tables =
  { 
    (* negative int -(n+1)  for Perform n, idx in tbl_trans for Shift *)
    tbl_base: int array;                 (* Perform / Shift *)
    (* -1 = No_remember, positive = Remember n *)
    tbl_backtrk: int array;              (* No_remember / Remember *)

    tbl_default: int array;              (* Default transition *)
    tbl_trans: int array;                (* Transitions (compacted) *)
    tbl_check: int array;                (* Check (compacted) *)
  }
@
%val compact_tables: Lexgen.automata_matrix -> lex_tables

% why -(n+1)? because can have (Perform 0) and (Shift 0)



\subsection{[[compact_tables()]]}

<<signature [[Compact.compact_tables]]>>=
val compact_tables: Lexgen.automata_matrix -> lex_tables
@


<<function [[Compact.compact_tables]]>>=
let compact_tables state_v =
  let n = Array.length state_v in

  let base = Array.make n 0 in
  let backtrk = Array.make n (-1) in
  let default = Array.make n 0 in


  for i = 0 to n - 1 do
    match state_v.(i) with
      Perform n ->
        base.(i) <- -(n+1)
    | Shift(trans, move) ->
        (match trans with
          No_remember -> ()
        | Remember n -> backtrk.(i) <- n
        );
        let (b, d) = pack_moves i move in
        base.(i) <- b;
        default.(i) <- d
  done;
  { tbl_base = base;
    tbl_backtrk = backtrk;
    tbl_default = default;
    tbl_trans = Array.sub !trans 0 !last_used;
    tbl_check = Array.sub !check 0 !last_used;
  }
@

<<global [[Compact.trans]]>>=
let trans = ref(Array.make 1024 0)
@
<<global [[Compact.check]]>>=
let check = ref(Array.make 1024 (-1))
@
<<global [[Compact.last_used]]>>=
let last_used = ref 0
@
% actually growing array, see grow_transition below



<<function [[Compact.pack_moves]]>>=
let pack_moves state_num move_t =
  let move_v = Array.make 257 0 in
  for i = 0 to 256 do
    move_v.(i) <-
      (match move_t.(i) with
        Backtrack -> -1
      | Goto n -> n)
  done;

  let default = most_frequent_elt move_v in
  let nondef = non_default_elements default move_v in

  let rec pack_from b =
    while b + 257 > Array.length !trans do 
      grow_transitions() 
    done;
    let rec try_pack = function
      [] -> b
    | (pos, _v) :: rem ->
        if !check.(b + pos) = -1 then try_pack rem else pack_from (b+1) in
    try_pack nondef 
  in
  let base = pack_from 0 in

  nondef |> List.iter (fun (pos, v) ->
      !trans.(base + pos) <- v;
      !check.(base + pos) <- state_num
  );
  if base + 257 > !last_used 
  then last_used := base + 257;
  (base, default)
@

<<function [[Compact.most_frequent_elt]]>>=
(* Determine the integer occurring most frequently in an array *)

let most_frequent_elt v =
  let frequencies = Hashtbl.create 17 in
  let max_freq = ref 0 in
  let most_freq = ref (v.(0)) in
  for i = 0 to Array.length v - 1 do
    let e = v.(i) in
    let r =
      try
        Hashtbl.find frequencies e
      with Not_found ->
        let r = ref 1 in Hashtbl.add frequencies e r; r 
    in
    incr r;
    if !r > !max_freq then begin 
      max_freq := !r; 
      most_freq := e 
    end
  done;
  !most_freq
@
% key compression optimisation


<<function [[Compact.non_default_elements]]>>=
(* Transform an array into a list of (position, non-default element) *)

let non_default_elements def v =
  let rec nondef i =
    if i >= Array.length v 
    then [] 
    else begin
      let e = v.(i) in
      if e = def then nondef(i+1) else (i, e) :: nondef(i+1)
    end in
  nondef 0
@




%less: could be in a stdlib
<<function [[Compact.grow_transitions]]>>=
let grow_transitions () =
  let old_trans = !trans
  and old_check = !check in
  let n = Array.length old_trans in
  trans := Array.make (2*n) 0;
  Array.blit old_trans 0 !trans 0 !last_used;
  check := Array.make (2*n) (-1);
  Array.blit old_check 0 !check 0 !last_used
@



\subsection{Output compacted tables}

<<type [[Lexing.lex_tables]]>>=
(* The following definitions are used by the generated scanners only.
   They are not intended to be used by user programs. *)

type lex_tables =
  { lex_base: string;
    lex_backtrk: string;
    lex_default: string;
    lex_trans: string;
    lex_check: string }
@

\subsection{C transition engine}
% another opti
%todo: bench

\section{Yacc}

% kernel items

% compaction

%TODO: DeRemer and Pennello algorithm

\subsection{C transition engine}
% another opti
%todo: bench

\chapter{Advanced Topics}

\section{Unicode}

% ulex? char_ is problematic but should be easy to change
% no? instead of get_next_char it would be get_next_unicode_char?


\chapter{Conclusion}

\appendix

\chapter{Debugging}

% can also use the debug of ocamllex to help debug ocamllex
% itself if you think the generated code is wrong and you want
% to trace it.

% plan9 yacc has a -D <n> option to set yydebug to n and
% then y.tab.c contains code looking for the value of yydebug.

\section{Dumpers}


<<signature [[Dump.dump_lr0_automaton]](yacc)>>=
val dump_lr0_automaton: Lr0.env -> Lr0.automaton -> unit
@

<<signature [[Dump.dump_item]](yacc)>>=
val dump_item: Lr0.env -> Lr0.item -> unit
@
<<signature [[Dump.dump_items]](yacc)>>=
val dump_items: Lr0.env -> Lr0.items -> unit
@



<<signature [[Dump.dump_lrtables]](yacc)>>=
val dump_lrtables: Lr0.env -> Lrtables.lr_tables -> unit
@


<<function [[Dump.string_of_action]](yacc)>>=
let string_of_action x =
  match x with
  | Lrtables.Shift (S d) -> spf "s%d" d
  | Lrtables.Reduce (R d) -> spf "r%d" d
  | Lrtables.Accept -> spf "acc"
  | Lrtables.Error -> ""
@


<<function [[Dump.dump_symbol]](yacc)>>=
let dump_symbol s =
  print_string (string_of_symbol s)
@

<<function [[Dump.string_of_symbol]](yacc)>>=
let string_of_symbol s =
  match s with
  (* in ocamlyacc terminals are constructors and so are uppercase and
   * so non terminals are lowercase, but it's the opposite convention
   * used in the dragon book, so here we dump in the dragon book
   * way so it's easier to compare what we generate with what
   * the dragon book says we should generate
   *)
  | Nonterm (NT s) -> String.uppercase_ascii s
  | Term (T s) ->
    (match s with
    (* special cases for arith.mly and tests.ml grammar toy examples *)
    | "PLUS" -> "+"
    | "MULT" -> "*"
    | "TOPAR" -> "("
    | "TCPAR" -> ")"

    | _ -> String.lowercase_ascii s
    )
@


<<function [[Dump.dump_item]](yacc)>>=
let dump_item env item =
  let (R idx, D didx) = item in
  let r = env.g.(idx) in

  dump_symbol (Nonterm r.lhs);
  print_space ();
  print_string "->";
  open_box 0;
  print_space ();
  r.rhs |> Array.of_list |> Array.iteri (fun i s ->
    if i = didx
    then begin 
      print_string ".";
      print_space ();
    end;
    dump_symbol s;
    print_space ();
  );
  if didx = List.length r.rhs then begin
      print_string ".";
  end;

  close_box ();
(*  print_space (); print_string "(R"; print_int idx; print_string ")" *)
  ()
@

<<function [[Dump.dump_items]](yacc)>>=
let dump_items env items =
  items 
  |> Set.elements |> List.sort (fun (R a, _) (R b, _) -> a - b)
  |> List.iter (fun item -> 
    open_box 0;
    dump_item env item;
    close_box ();
    print_newline ();
  )
@

<<function [[Dump.dump_lr0_automaton]](yacc)>>=
let dump_lr0_automaton env auto =

  open_box 0;

  (* the states *)
  auto.int_to_state |> Array.iteri (fun i items ->
    print_string "I"; print_int i; print_newline ();
    open_box 2;
    dump_items env items;
    close_box ();
    print_newline ();
  );

  (* the transitions *)
  auto.trans |> Map.iter (fun (items1, symb) items2 ->
    let (S src) = Map.find items1 auto.state_to_int in
    let (S dst) = Map.find items2 auto.state_to_int in
    print_string "I"; print_int src;
    print_string " --"; dump_symbol symb; print_string "-->";
    print_string " I"; print_int dst;
    print_newline ()
  );

  close_box ()
@

<<function [[Dump.dump_lrtables]](yacc)>>=
let dump_lrtables env lrtables =
  let symbols = Lr0.all_symbols env in
  let (action_table, goto_table) = lrtables in
  let haction = hash_of_list action_table in
  let hgoto = hash_of_list goto_table in

  let (terms, nonterms) = 
    symbols |> Set.elements |> partition_either (function
      | Term t -> Left t
      | Nonterm nt -> Right nt
    )
  in
  let terms = terms @ [Ast.dollar_terminal] in
  let max_state = 
    action_table |> List.fold_left (fun acc (((S stateid), _), _) ->
      max acc stateid
    ) 0
  in
  
  (* print headers *)
  pf "   ";
  terms |> List.iter (fun t ->
    let s = string_of_symbol (Term t) in
    pf "%3s " s;
  );
  pf "  ";
  nonterms |> List.iter (fun nt ->
    let s = string_of_symbol (Nonterm nt) in
    pf "%3s " s;
  );
  pf "\n";

  let conflicts = ref [] in
  
  for i = 0 to max_state do
    pf "%2d " i;
    terms |> List.iter (fun t ->
      let xs = Hashtbl.find_all haction (S i, t) in
      (match xs with
      | [] -> pf "%3s " " "
      | [x] -> pf "%3s " (string_of_action x)
      | x::xs -> 
        pf "%3s " "!?!";
        conflicts := (S i, t, x::xs)::!conflicts
      );
    );
    pf "  ";
    nonterms |> List.iter (fun nt ->
      let xs = Hashtbl.find_all hgoto (S i, nt) in
      (match xs with
      | [] -> pf "%3s " " "
      | [S d] -> pf "%3d " d
      | _x::_xs -> pf "%3s " "!?!"
      );
    );

    pf "\n";
  done;
  pf "\n";
  pf "%d conflicts\n" (List.length !conflicts);
  pf "\n";
  ()
@


\chapter{Error Management}




% Main of lex
<<[[Main.main()]] report error exn>>=
(match exn with
  Parsing.Parse_error ->
    prerr_string "Syntax error around char ";
    prerr_int (Lexing.lexeme_start lexbuf);
    prerr_endline "."
| Lexer.Lexical_error s ->
    prerr_string "Lexical error around char ";
    prerr_int (Lexing.lexeme_start lexbuf);
    prerr_string ": ";
    prerr_string s;
    prerr_endline "."
| _ -> raise exn
);
@


<<function [[Check.report_error]](yacc)>>=
let report_error err =
  failwith "TODO"
@


\chapter{Utilities}

<<function [[Slr.filter_some]](yacc)>>=
(* from my common.ml *)
let rec filter_some = function
  | [] -> []
  | None :: l -> filter_some l
  | Some e :: l -> e :: filter_some l
@

<<function [[Slr.map_filter]](yacc)>>=
let map_filter f xs = xs |> List.map f |> filter_some
@


<<type [[Dump.either]](yacc)>>=
type ('a,'b) either = Left of 'a | Right of 'b
@

<<function [[Dump.partition_either]](yacc)>>=
let partition_either f l =
  let rec part_either left right = function
  | [] -> (List.rev left, List.rev right)
  | x :: l ->
      (match f x with
      | Left  e -> part_either (e :: left) right l
      | Right e -> part_either left (e :: right) l) in
  part_either [] [] l
@

<<function [[Dump.hash_of_list]](yacc)>>=
let hash_of_list xs =
  let h = Hashtbl.create 101 in
  xs |> List.iter (fun (k, v) -> Hashtbl.replace h k v);
  h
@

<<constant [[Output.spf]](yacc)>>=
let spf = Printf.sprintf
@

<<constant [[Dump.spf]](yacc)>>=
let spf = Printf.sprintf
@

<<constant [[Parsing.spf]](yacc)>>=
let spf = Printf.sprintf
@

<<constant [[Dump.pf]](yacc)>>=
let pf = Printf.printf
@

\chapter{Extra Code}

#include "CompilerGenerator_extra.nw"

%\chapter{Changelog}
%\label{sec:changelog}

\chapter*{Glossary}
\addcontentsline{toc}{chapter}{Glossary}
\label{sec:glossary}

\begin{verbatim}

\end{verbatim}

\chapter*{Indexes}
\addcontentsline{toc}{chapter}{Indexes}

%src: wc.nw in noweb source
Here is a list of the identifiers used, and where they appear.
Underlined entries indicate the place of definition.
This index is generated automatically.

%\twocolumn does not work
\nowebindex

%\chapter{References} 
\addcontentsline{toc}{chapter}{References}

\bibliography{../docs/latex/Principia}
\bibliographystyle{plain}

%******************************************************************************
% Postlude
%******************************************************************************

\end{document}
